{"cells":[{"cell_type":"markdown","metadata":{"id":"Vu8EdO6H3r5T"},"source":["## Downloading packages"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15065,"status":"ok","timestamp":1747893565071,"user":{"displayName":"Filip Nykvist","userId":"11309094825996781814"},"user_tz":-120},"id":"-D9ztHeLblQv","outputId":"60bbf144-b824-456c-b9f8-284ced82aa8b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting trimesh\n","  Downloading trimesh-4.6.10-py3-none-any.whl.metadata (18 kB)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.11/dist-packages (from trimesh) (2.0.2)\n","Downloading trimesh-4.6.10-py3-none-any.whl (711 kB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/711.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m711.2/711.2 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: trimesh\n","Successfully installed trimesh-4.6.10\n","Requirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (7.7.1)\n","Collecting ipywidgets\n","  Downloading ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\n","Collecting comm>=0.1.3 (from ipywidgets)\n","  Downloading comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n","Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (7.34.0)\n","Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (5.7.1)\n","Collecting widgetsnbextension~=4.0.14 (from ipywidgets)\n","  Downloading widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n","Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (3.0.15)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (75.2.0)\n","Collecting jedi>=0.16 (from ipython>=6.1.0->ipywidgets)\n","  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n","Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.13)\n","Downloading ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n","Downloading widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: widgetsnbextension, jedi, comm, ipywidgets\n","  Attempting uninstall: widgetsnbextension\n","    Found existing installation: widgetsnbextension 3.6.10\n","    Uninstalling widgetsnbextension-3.6.10:\n","      Successfully uninstalled widgetsnbextension-3.6.10\n","  Attempting uninstall: ipywidgets\n","    Found existing installation: ipywidgets 7.7.1\n","    Uninstalling ipywidgets-7.7.1:\n","      Successfully uninstalled ipywidgets-7.7.1\n","Successfully installed comm-0.2.2 ipywidgets-8.1.7 jedi-0.19.2 widgetsnbextension-4.0.14\n"]}],"source":["!pip install trimesh\n","!pip install --upgrade ipywidgets"]},{"cell_type":"markdown","metadata":{"id":"q9iQ9GE_3wQG"},"source":["## Mounting to Google Drive"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12569,"status":"ok","timestamp":1747893964701,"user":{"displayName":"Filip Nykvist","userId":"11309094825996781814"},"user_tz":-120},"id":"xeacSnj630HZ","outputId":"173c00ac-299a-45b4-ef13-75e965e14c38"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["import os\n","from google.colab import drive\n","\n","# Define the base path for your Google Drive.\n","base_path = '/content/drive'\n","\n","# Define the specific folder path within your Google Drive.\n","folder_path = 'MyDrive/Colab Notebooks/Machine Learning and Deep Learning/Project'\n","\n","# Combine the base path and folder path to create the full mount path.\n","full_project_path = os.path.join(base_path, folder_path)\n","\n","# Mount your drive.\n","drive.mount(base_path, force_remount=True)"]},{"cell_type":"markdown","metadata":{"id":"KPyo8_eyCgyr"},"source":["## Creating custom dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"68wVlpfdCjJi"},"outputs":[],"source":["import os\n","import yaml\n","import numpy as np\n","import json\n","import torch\n","import trimesh\n","from torch.utils.data import Dataset\n","from torchvision import transforms\n","from sklearn.model_selection import train_test_split\n","from PIL import Image\n","\n","class PoseEstimationDataset_RGBD(Dataset):\n","    \"\"\"\n","    Custom PyTorch Dataset class for loading RGB-D images and pose annotations\n","    from the LineMOD dataset for 6D object pose estimation tasks.\n","    \"\"\"\n","    def __init__(self, dataset_root,\n","                 models_root,\n","                 folders=list(range(1, 16)),\n","                 split='train',\n","                 train_ratio=0.8,\n","                 seed=42,\n","                 img_size=(224, 224)):\n","        \"\"\"\n","        Initialize dataset by loading metadata and preparing the image/pose samples.\n","\n","        Args:\n","            dataset_root (str): Path to the root of the RGB-D dataset.\n","            models_root (str): Path to the directory containing 3D object models and models_info.yml.\n","            folders (list): List of object folders to include.\n","            split (str): 'train' or 'val' split.\n","            train_ratio (float): Ratio of samples to include in the training set.\n","            seed (int): Random seed for train/test split reproducibility.\n","            img_size (tuple): Target image size for network input.\n","        \"\"\"\n","        self.dataset_root = dataset_root\n","        self.models_root = models_root\n","        self.split = split\n","        self.train_ratio = train_ratio\n","        self.seed = seed\n","        self.img_size = img_size\n","        self.models = {}\n","        self.invalid_entries = 0\n","\n","        # Load model metadata (e.g., object dimensions) from YAML.\n","        self.models_info_path = os.path.join(models_root, 'models_info.yml')\n","        with open(self.models_info_path, 'r') as f:\n","            self.models_info = yaml.safe_load(f)\n","\n","        # Chaching for effectice loading.\n","        self.gt_data = {}    # Ground truth poses.\n","        self.info_data = {}  # Camera intrinsics.\n","        self.all_samples = []  # All available (object_id, sample_id) pairs.\n","\n","        # Load gt.yml and info.yml for each object folder.\n","        for obj_id in folders:\n","            obj_folder = os.path.join(dataset_root, f\"{obj_id:02d}\")\n","            gt_path = os.path.join(obj_folder, 'gt.yml')\n","            info_path = os.path.join(obj_folder, 'info.yml')\n","\n","            if not os.path.exists(gt_path):\n","                continue\n","\n","            with open(gt_path, 'r') as f:\n","                gt = yaml.safe_load(f)\n","            with open(info_path, 'r') as f:\n","                info = yaml.safe_load(f)\n","\n","            self.gt_data[obj_id] = gt\n","            self.info_data[obj_id] = info\n","\n","            for sample_id in gt.keys():\n","                self.all_samples.append((obj_id, int(sample_id)))\n","\n","        # Building a mapping between original and internal object IDs.\n","        object_id_set = set()\n","        for obj_id, sample_id in self.all_samples:\n","            annotations = self.gt_data[obj_id][sample_id]\n","            for ann in annotations:\n","                object_id_set.add(ann['obj_id'])\n","        self.object_ids = sorted(object_id_set)\n","        self.id_to_idx = {obj_id: i for i, obj_id in enumerate(self.object_ids)}\n","        self.idx_to_id = {i: obj_id for obj_id, i in self.id_to_idx.items()}\n","\n","        if not self.all_samples:\n","            raise ValueError(f\"No samples found in {dataset_root}. Check dataset structure.\")\n","\n","        # Split dataset into training and test sets.\n","        train, test = train_test_split(self.all_samples, train_size=self.train_ratio, random_state=self.seed)\n","        self.samples = train if self.split == 'train' else test\n","\n","        # Image preprocessing pipeline.\n","        self.transform = transforms.Compose([\n","            transforms.Resize(self.img_size),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                 std=[0.229, 0.224, 0.225])\n","        ])\n","\n","    def __len__(self):\n","        \"\"\"Return the number of samples in the dataset.\"\"\"\n","        return len(self.samples)\n","\n","    def save_mapping(self, filepath=None):\n","        \"\"\"Save object ID to index mapping as JSON.\"\"\"\n","        if filepath is None:\n","            filepath = \"object_id_mapping.json\"\n","        with open(filepath, \"w\") as f:\n","            json.dump(self.id_to_idx, f)\n","\n","    @staticmethod\n","    def load_mapping(filepath=None):\n","        \"\"\"Load object ID to index mapping from JSON file.\"\"\"\n","        if filepath is None:\n","            filepath = \"object_id_mapping.json\"\n","        with open(filepath, \"r\") as f:\n","            mapping = json.load(f)\n","        return {int(k): v for k, v in mapping.items()}\n","\n","    def printIDMapping(self):\n","        \"\"\"Print human-readable object ID mapping.\"\"\"\n","        print(\"Object ID Mapping (Original → Mapped):\")\n","        for orig_id in self.object_ids:\n","            print(f\"  {orig_id:02d} → {self.id_to_idx[orig_id]}\")\n","\n","    def nrInvalidObjects(self):\n","        \"\"\"Return number of filtered-out invalid object crops.\"\"\"\n","        return self.invalid_entries\n","\n","    def getMappedIDs(self, ids=None):\n","        \"\"\"\n","        Maps original object IDs to their training indices.\n","\n","        Args:\n","            ids (list or None): Original object IDs. If None, returns all mappings.\n","        Returns:\n","            mapped_ids (list): Mapped training indices.\n","            orig_ids (list): Corresponding original object IDs.\n","        \"\"\"\n","        mapped_ids = []\n","        orig_ids = []\n","        if ids is None:\n","            for orig_id in self.object_ids:\n","                mapped_ids.append(self.id_to_idx[orig_id])\n","                orig_ids.append(orig_id)\n","        else:\n","            for id in ids:\n","                if id in self.id_to_idx:\n","                    mapped_ids.append(self.id_to_idx[id])\n","                    orig_ids.append(id)\n","                else:\n","                    print(f\"⚠️ Warning: Object ID {id} not found in dataset and will be ignored.\")\n","        return mapped_ids, orig_ids\n","\n","    def get_model_info(self, object_id):\n","        \"\"\"Returns metadata (ex. diameter) for the specified object ID.\"\"\"\n","        if object_id not in self.models_info:\n","            raise ValueError(f\"Object ID {object_id} not in models_info.yml\")\n","        return self.models_info[object_id]\n","\n","    def load_3D_model(self, object_id):\n","        \"\"\"\n","        Loads and returns the 3D model (in meters) of the specified object.\n","        Caches the result to avoid redundant loading.\n","        \"\"\"\n","        if object_id in self.models:\n","            return self.models[object_id]\n","\n","        model_path = os.path.join(self.models_root, f\"obj_{object_id:02d}.ply\")\n","        mesh = trimesh.load(model_path)\n","        points = mesh.vertices.astype(np.float32) / 1000.0\n","        self.models[object_id] = points\n","        return points\n","\n","    def cropImages(self, image, annotations, cam_K, depth_image=None):\n","        \"\"\"\n","        Crops object regions from the image (rgb + depth) based on bounding boxes and adjusts the intrinsics.\n","\n","        Args:\n","            image (PIL.Image): Original RGB image.\n","            annotations (list): Object annotations from gt.yml.\n","            cam_K (torch.Tensor): Original 3x3 camera intrinsic matrix.\n","            depth_image (PIL.Image or None): depth image.\n","\n","        Returns:\n","            crop_entries (list): List of cropped image entries with adjusted intrinsics.\n","            cam_K (torch.Tensor): Original intrinsic matrix.\n","        \"\"\"\n","        crop_entries = []\n","        width, height = image.size\n","\n","        for ann in annotations:\n","            x, y, w, h = ann['obj_bb']\n","            x1 = max(0, x)\n","            y1 = max(0, y)\n","            x2 = min(width, x + w)\n","            y2 = min(height, y + h)\n","\n","            if x2 <= x1 or y2 <= y1:\n","                self.invalid_entries += 1\n","                continue\n","\n","            cropped = image.crop((x1, y1, x2, y2))\n","            original_crop_width, original_crop_height = cropped.size\n","\n","            # Adjust camera intrinsics for the cropped region.\n","            cropped_K = cam_K.clone()\n","            cropped_K[0, 2] -= x1\n","            cropped_K[1, 2] -= y1\n","\n","            # Resize crop and update intrinsics accordingly.\n","            cropped = cropped.resize(self.img_size, Image.BILINEAR)\n","            scale_x = self.img_size[0] / original_crop_width\n","            scale_y = self.img_size[1] / original_crop_height\n","            cropped_K[0, 0] *= scale_x\n","            cropped_K[0, 2] *= scale_x\n","            cropped_K[1, 1] *= scale_y\n","            cropped_K[1, 2] *= scale_y\n","\n","            cropped_rgb_tensor = self.transform(cropped)\n","\n","            # Extract pose (rotation, translation).\n","            R_mat = np.array(ann['cam_R_m2c'], dtype=np.float32).reshape(3, 3)\n","            t_vec = np.array(ann['cam_t_m2c'], dtype=np.float32) / 1000.0\n","\n","            # Handle optional depth cropping.\n","            if depth_image is not None:\n","                cropped_depth = depth_image.crop((x1, y1, x2, y2))\n","                cropped_depth = cropped_depth.resize(self.img_size, Image.BILINEAR)\n","                cropped_depth_tensor = transforms.ToTensor()(cropped_depth)\n","            else:\n","                cropped_depth_tensor = None\n","\n","            # Normalize bounding box coordinates to [0, 1].\n","            norm_bbox = torch.tensor([\n","                x1 / width,\n","                y1 / height,\n","                (x2 - x1) / width,\n","                (y2 - y1) / height\n","            ], dtype=torch.float32)\n","\n","            crop_entries.append({\n","                'cropped_rgb': cropped_rgb_tensor,\n","                'cropped_depth': cropped_depth_tensor,\n","                'cropped_K': cropped_K,\n","                'object_id': ann['obj_id'],\n","                'bbox': ann['obj_bb'],\n","                'norm_bbox': norm_bbox,\n","                'rotation': R_mat,\n","                'translation': t_vec\n","            })\n","\n","        return crop_entries, cam_K\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        Load an RGB-D sample and return a dict with:\n","        - Original RGB and depth image\n","        - Cropped objects with pose and camera intrinsics\n","        \"\"\"\n","        object_id, sample_id = self.samples[idx]\n","        folder = os.path.join(self.dataset_root, f\"{object_id:02d}\")\n","\n","        rgb_path = os.path.join(folder, 'rgb', f\"{sample_id:04d}.png\")\n","        if not os.path.exists(rgb_path):\n","            raise FileNotFoundError(f\"RGB image not found: {rgb_path}\")\n","        rgb = Image.open(rgb_path).convert(\"RGB\")\n","\n","        depth_path = os.path.join(folder, 'depth', f\"{sample_id:04d}.png\")\n","        if not os.path.exists(depth_path):\n","            raise FileNotFoundError(f\"Depth image not found: {depth_path}\")\n","        depth = Image.open(depth_path)\n","        depth_np = np.array(depth).astype(np.float32)\n","        depth_np = np.clip(depth_np, 0, 2000) / 2000.0 # Normalizing depth image between [0, 2].\n","        depth_img = Image.fromarray((depth_np * 255).astype(np.uint8))\n","\n","        annotations = self.gt_data[object_id][sample_id]\n","        cam_K = torch.tensor(np.array(self.info_data[object_id][sample_id]['cam_K']).reshape(3, 3), dtype=torch.float32)\n","\n","        crop_entries, org_K = self.cropImages(rgb, annotations, cam_K, depth_image=depth_img)\n","\n","        # Map object IDs to internal indices.\n","        for entry in crop_entries:\n","            true_obj_id = entry['object_id']\n","            if true_obj_id not in self.id_to_idx:\n","                raise ValueError(f\"Object ID {true_obj_id} not found in id_to_idx mapping.\")\n","            entry['object_id'] = self.id_to_idx[true_obj_id]\n","\n","        return {\n","            'sample_id': sample_id,\n","            'original_rgb': rgb,\n","            'original_K': org_K,\n","            'original_depth': depth_np,\n","            'objects': crop_entries\n","        }"]},{"cell_type":"markdown","metadata":{"id":"87dUPITNvUGI"},"source":["## PoseNet6D using RGB-D data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tov6Nf4mvZdj"},"outputs":[],"source":["import torch.nn.functional as F\n","from torchvision.models import resnet18, ResNet18_Weights\n","from torch import nn\n","\n","\n","class PoseNet_RGBD(nn.Module):\n","    \"\"\"\n","    Neural network for 6D object pose estimation using RGB-D input.\n","    Takes cropped RGB-D images, normalized 2D bounding box, intrinsic matrix,\n","    and object ID to predict rotation (quaternion) and translation (3D position).\n","    \"\"\"\n","    def __init__(self, num_objects,\n","                 embedding_dim=16,\n","                 img_size=(224, 224),\n","                 weights=ResNet18_Weights.DEFAULT):\n","        \"\"\"\n","        Initialize PoseNet_RGBD architecture.\n","\n","        Args:\n","            num_objects (int): Number of distinct object classes.\n","            embedding_dim (int): Size of the object ID embedding vector.\n","            img_size (tuple): Size of input RGB and depth crops.\n","            weights (ResNet18_Weights): Pretrained weights for ResNet18 RGB encoder.\n","        \"\"\"\n","        super().__init__()\n","        self.img_size = img_size\n","\n","        # === RGB Encoder ===\n","        # Use pretrained ResNet18, discard final classification layers.\n","        rgb_backbone = resnet18(weights=weights)\n","        self.rgb_encoder = nn.Sequential(*list(rgb_backbone.children())[:-2])  # output: (B, 512, 7, 7).\n","        self.global_pool_rgb = nn.AdaptiveAvgPool2d(1)  # convert from (B, 512, 1, 1) to (B, 512).\n","\n","        # === Depth Encoder ===\n","        # A lightweight CNN for single-channel depth input.\n","        self.depth_encoder = nn.Sequential(\n","            nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(),\n","            nn.MaxPool2d(2),  # -> 112x112\n","            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n","            nn.MaxPool2d(2),  # -> 56x56\n","            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(),\n","            nn.AdaptiveAvgPool2d(1),  # -> (B, 128, 1, 1)\n","            nn.Flatten()              # -> (B, 128)\n","        )\n","\n","        # === Object ID Embedding ===\n","        # Learn a small dense vector representation for each object ID.\n","        self.obj_embedding = nn.Embedding(num_embeddings=num_objects,\n","                                          embedding_dim=embedding_dim)\n","\n","        # === Feature Fusion ===\n","        # Concatenate RGB + depth + bbox + obj_embedding vectors.\n","        fused_dim = 512 + 128 + 4 + embedding_dim\n","\n","        # Fully connected layers for depth (Z translation) prediction.\n","        self.fc_depth = nn.Sequential(\n","            nn.Linear(fused_dim, 256),\n","            nn.BatchNorm1d(256),\n","            nn.ReLU(),\n","            nn.Linear(256, 1)\n","        )\n","\n","        # Fully connected layers for rotation (quaternion) prediction.\n","        self.fc_rotation = nn.Sequential(\n","            nn.Linear(fused_dim, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 4)  # Quaternion output.\n","        )\n","\n","    def forward(self, x_rgb, x_depth, norm_bbox, K_crop, object_id):\n","        \"\"\"\n","        Forward pass to predict 6D pose.\n","\n","        Args:\n","            x_rgb (Tensor): RGB image crop (B, 3, H, W)\n","            x_depth (Tensor): Depth image crop (B, 1, H, W)\n","            norm_bbox (Tensor): Normalized bounding boxes (B, 4)\n","            K_crop (Tensor): Camera intrinsics for cropped view (B, 3, 3)\n","            object_id (Tensor): Object class IDs (B,)\n","\n","        Returns:\n","            translation (Tensor): Predicted 3D translation vector (B, 3)\n","            quat (Tensor): Predicted rotation as quaternion (B, 4)\n","        \"\"\"\n","        B = x_rgb.shape[0]\n","\n","        # === Extract RGB features ===\n","        rgb_feat = self.rgb_encoder(x_rgb)                      # (B, 512, 7, 7)\n","        rgb_feat = self.global_pool_rgb(rgb_feat).view(B, -1)   # (B, 512)\n","\n","        # === Extract depth features ===\n","        depth_feat = self.depth_encoder(x_depth)                # (B, 128)\n","\n","        # === Object ID embedding ===\n","        object_id = object_id.to(self.obj_embedding.weight.device)\n","        obj_feat = self.obj_embedding(object_id)                # (B, embedding_dim)\n","\n","        # === Concatenate all features ===\n","        x = torch.cat([rgb_feat, depth_feat, norm_bbox, obj_feat], dim=1)  # (B, fused_dim)\n","\n","        # === Predict object depth (Z translation) ===\n","        depth = self.fc_depth(x).squeeze(1)                     # (B,)\n","        depth = torch.clamp(depth, min=0.1, max=1.5)            # limit predictions to valid range.\n","\n","        # === Predict rotation as normalized quaternion ===\n","        quat = F.normalize(self.fc_rotation(x), dim=1)          # (B, 4)\n","\n","        # === Compute translation (X, Y, Z) in camera frame ===\n","        # Extract intrinsic parameters from cropped camera intrinsics.\n","        fx = K_crop[:, 0, 0]       # Focal length in x-direction.\n","        fy = K_crop[:, 1, 1]       # Focal length in y-direction.\n","        cx_crop = K_crop[:, 0, 2]  # Principal point x-coordinate (cropped).\n","        cy_crop = K_crop[:, 1, 2]  # Principal point y-coordinate (cropped).\n","\n","        # Compute center of bounding box in pixel coordinates.\n","        img_w, img_h = self.img_size\n","        u = (norm_bbox[:, 0] + 0.5 * norm_bbox[:, 2]) * img_w  # Horizontal center (in pixels).\n","        v = (norm_bbox[:, 1] + 0.5 * norm_bbox[:, 3]) * img_h  # Vertical center (in pixels).\n","\n","        # Back-project 2D bbox center + predicted depth to 3D camera coordinates.\n","        x_cam = (u - cx_crop) * depth / fx  # X = (u - cx) * Z / fx.\n","        y_cam = (v - cy_crop) * depth / fy  # Y = (v - cy) * Z / fy.\n","        z_cam = depth                       # Z is directly predicted.\n","\n","        # Stack into final 3D translation vector (X, Y, Z).\n","        translation = torch.stack([x_cam, y_cam, z_cam], dim=1)  # (B, 3)\n","\n","        return translation, quat"]},{"cell_type":"markdown","metadata":{"id":"keS9PBYASWjI"},"source":["## Helper functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9zPtmURiSYNs"},"outputs":[],"source":["import torch.nn.functional as F\n","from scipy.spatial.transform import Rotation as R\n","from scipy.spatial import cKDTree\n","\n","def computeADD(R_pred, t_pred, R_gt, t_gt, model_points):\n","    \"\"\"\n","    Computes the ADD (Average Distance of Model Points) metric.\n","\n","    Args:\n","        R_pred (np.ndarray or Tensor): Predicted rotation matrix (3x3).\n","        t_pred (np.ndarray or Tensor): Predicted translation vector (3,).\n","        R_gt (np.ndarray or Tensor): Ground truth rotation matrix (3x3).\n","        t_gt (np.ndarray or Tensor): Ground truth translation vector (3,).\n","        model_points (np.ndarray or Tensor): 3D object model points (N, 3).\n","\n","    Returns:\n","        float: Mean Euclidean distance between transformed predicted and ground truth points.\n","    \"\"\"\n","    def to_np(x):\n","        return x.detach().cpu().numpy() if isinstance(x, torch.Tensor) else x\n","\n","    # Convert all inputs to NumPy arrays.\n","    R_pred, t_pred = to_np(R_pred), to_np(t_pred)\n","    R_gt, t_gt = to_np(R_gt), to_np(t_gt)\n","    model_points = to_np(model_points)\n","\n","    # Apply transformations to model points.\n","    pred_pts = model_points @ R_pred.T + t_pred\n","    gt_pts = model_points @ R_gt.T + t_gt\n","\n","    # Compute mean L2 distance.\n","    distances = np.linalg.norm(pred_pts - gt_pts, axis=1)\n","    return distances.mean()\n","\n","def matrix_to_quaternion_batch(rotation_matrix):\n","    \"\"\"\n","    Converts a batch of rotation matrices to quaternions.\n","\n","    Args:\n","        rotation_matrix (Tensor): (B, 3, 3) or (3, 3) rotation matrix.\n","\n","    Returns:\n","        Tensor: (B, 4) batch of normalized quaternions (w, x, y, z).\n","    \"\"\"\n","    if rotation_matrix.dim() == 2:\n","        rotation_matrix = rotation_matrix.unsqueeze(0)  # Ensure batch dimension.\n","\n","    batch_size = rotation_matrix.size(0)\n","    quaternions = torch.zeros(batch_size, 4, device=rotation_matrix.device)\n","\n","    # Compute trace for each matrix.\n","    trace = torch.diagonal(rotation_matrix, dim1=1, dim2=2).sum(dim=1)\n","    trace = torch.clamp(trace, min=-0.999)  # Stability clamp\n","\n","    # Compute scalar (w) part.\n","    s = torch.sqrt(1.0 + trace) / 2.0\n","    quaternions[:, 0] = s\n","\n","    # Prevent division by zero.\n","    denom = 4.0 * s\n","    denom = torch.clamp(denom, min=1e-6)\n","\n","    # Compute vector part (x, y, z).\n","    quaternions[:, 1] = (rotation_matrix[:, 2, 1] - rotation_matrix[:, 1, 2]) / denom\n","    quaternions[:, 2] = (rotation_matrix[:, 0, 2] - rotation_matrix[:, 2, 0]) / denom\n","    quaternions[:, 3] = (rotation_matrix[:, 1, 0] - rotation_matrix[:, 0, 1]) / denom\n","\n","    # Normalize result quaternion.\n","    return F.normalize(quaternions, dim=1)\n","\n","def quaternion_to_matrix_batch(quat):\n","    \"\"\"\n","    Converts a batch of quaternions to rotation matrices.\n","\n","    Args:\n","        quat (Tensor): (B, 4) or (4,) quaternions in (w, x, y, z) format.\n","\n","    Returns:\n","        Tensor: (B, 3, 3) batch of rotation matrices.\n","    \"\"\"\n","    if quat.dim() == 1:\n","        quat = quat.unsqueeze(0)  # Ensure batch dimension.\n","\n","    w, x, y, z = quat[:, 0], quat[:, 1], quat[:, 2], quat[:, 3]\n","    B = quat.size(0)\n","    R = torch.zeros((B, 3, 3), device=quat.device)\n","\n","    # Populate rotation matrix elements from quaternion.\n","    R[:, 0, 0] = 1 - 2 * (y**2 + z**2)\n","    R[:, 0, 1] = 2 * (x * y - z * w)\n","    R[:, 0, 2] = 2 * (x * z + y * w)\n","    R[:, 1, 0] = 2 * (x * y + z * w)\n","    R[:, 1, 1] = 1 - 2 * (x**2 + z**2)\n","    R[:, 1, 2] = 2 * (y * z - x * w)\n","    R[:, 2, 0] = 2 * (x * z - y * w)\n","    R[:, 2, 1] = 2 * (y * z + x * w)\n","    R[:, 2, 2] = 1 - 2 * (x**2 + y**2)\n","\n","    return R.squeeze(0) if quat.size(0) == 1 else R\n","\n","def quaternion_loss(quat_pred, quat_gt):\n","    \"\"\"\n","    Computes quaternion-based rotation loss.\n","\n","    Args:\n","        quat_pred (Tensor): (B, 4) predicted quaternions.\n","        quat_gt (Tensor): (B, 4) ground truth quaternions.\n","\n","    Returns:\n","        Tensor: Scalar loss measuring angular difference.\n","    \"\"\"\n","    quat_pred = F.normalize(quat_pred, dim=1)\n","    quat_gt = F.normalize(quat_gt, dim=1)\n","\n","    # Dot product gives cosine of half-angle between quaternions.\n","    dot = torch.sum(quat_pred * quat_gt, dim=1)\n","    dot = torch.clamp(dot, -1.0 + 1e-4, 1.0 - 1e-4)\n","\n","    # Loss is 1 - cos²(theta) to minimize angular error.\n","    return (1 - dot**2).mean()\n","\n","def quaternion_angular_error(q1, q2):\n","    \"\"\"\n","    Computes angular error between two batches of quaternions.\n","\n","    Args:\n","        q1 (Tensor): (B, 4) predicted quaternions.\n","        q2 (Tensor): (B, 4) ground truth quaternions.\n","\n","    Returns:\n","        Tensor: (B,) angular error in degrees.\n","    \"\"\"\n","    dot = torch.sum(q1 * q2, dim=1).clamp(-1.0, 1.0)\n","    dot = torch.abs(dot)  # Resolves the ±q ambiguity.\n","    angle = 2 * torch.acos(dot) * (180.0 / torch.pi)\n","    return angle\n","\n","def computeMSE(rot_pred, t_pred, rot_gt, t_gt, quat=False,\n","               weight_xyz=(1.0, 1.0, 0.1), beta=1.0, print_mse=False):\n","    \"\"\"\n","    Computes combined MSE loss for translation and rotation.\n","\n","    Args:\n","        rot_pred (Tensor): (B, 4) or (B, 3, 3) predicted rotation.\n","        t_pred (Tensor): (B, 3) predicted translation.\n","        rot_gt (Tensor): ground truth rotation.\n","        t_gt (Tensor): ground truth translation.\n","        quat (bool): If True, use quaternion loss; else use matrix loss.\n","        weight_xyz (tuple): Weights for (x, y, z) translation axes.\n","        beta (float): Weight for rotation loss term.\n","        print_mse (bool): If True, print detailed loss values.\n","\n","    Returns:\n","        tuple: (total_loss, (x_loss, y_loss, z_loss, angle_deg))\n","    \"\"\"\n","    # Clamp Z translation to avoid numerical instability in log.\n","    t_pred = torch.clamp(t_pred, min=1e-3)\n","    t_gt = torch.clamp(t_gt, min=1e-3)\n","\n","    # Weighted MSE translation loss (with log scaling for Z).\n","    x_loss = F.mse_loss(t_pred[:, 0], t_gt[:, 0]) * weight_xyz[0]\n","    y_loss = F.mse_loss(t_pred[:, 1], t_gt[:, 1]) * weight_xyz[1]\n","    z_loss = F.mse_loss(torch.log(t_pred[:, 2]), torch.log(t_gt[:, 2])) * weight_xyz[2]\n","    translation_loss = x_loss + y_loss + z_loss\n","\n","    # Rotation loss: quaternion or matrix.\n","    if quat:\n","        rotation_loss = quaternion_loss(rot_pred, rot_gt)\n","    else:\n","        rot_diff = torch.bmm(rot_pred.transpose(1, 2), rot_gt)\n","        identity = torch.eye(3, device=rot_pred.device).unsqueeze(0).expand(rot_pred.size(0), -1, -1)\n","        rotation_loss = F.mse_loss(rot_diff, identity)\n","\n","    # Combine the losses.\n","    total_loss = translation_loss + beta * rotation_loss\n","\n","    # Optional logging.\n","    angle_deg = None\n","    if print_mse:\n","        print(f\"\\nX loss:           {x_loss:.6f}\")\n","        print(f\"Y loss:           {y_loss:.6f}\")\n","        print(f\"Z loss:           {z_loss:.6f}\")\n","        print(f\"Rotation loss:    {rotation_loss:.6f}\")\n","        print(f\"Total loss:       {total_loss:.6f}\")\n","        if quat:\n","            angle_deg = quaternion_angular_error(rot_pred, rot_gt).mean().item()\n","            print(f\"Angular error (deg): {angle_deg:.2f}\")\n","\n","    return total_loss, (x_loss, y_loss, z_loss, angle_deg)\n","\n","def flatten_collate_fn(batch):\n","    \"\"\"\n","    Custom collate function for DataLoader to flatten all object crops\n","    from each scene into a single batch.\n","\n","    Args:\n","        batch (list): List of dicts from __getitem__ containing 'objects' lists.\n","\n","    Returns:\n","        dict: Batched tensors for rgb, depth, rotation, translation, etc.\n","    \"\"\"\n","    flat_data = []\n","\n","    # Iterate over samples and flatten all object entries.\n","    for sample in batch:\n","        for obj in sample['objects']:\n","            flat_data.append({\n","                'rgb': obj['cropped_rgb'],\n","                'depth': obj['cropped_depth'],\n","                'rotation': torch.tensor(obj['rotation'], dtype=torch.float32),\n","                'translation': torch.tensor(obj['translation'], dtype=torch.float32),\n","                'object_id': obj['object_id'],\n","                'bbox': obj['norm_bbox'].clone().detach(),\n","                'cropped_K': obj['cropped_K'].clone().detach()\n","            })\n","\n","    # Stack each field into batch tensors.\n","    rgb = torch.stack([item['rgb'] for item in flat_data])\n","    depth = torch.stack([item['depth'] for item in flat_data])\n","    rotation = torch.stack([item['rotation'] for item in flat_data])\n","    translation = torch.stack([item['translation'] for item in flat_data])\n","    object_ids = torch.tensor([item['object_id'] for item in flat_data], dtype=torch.int64)\n","    bbox = torch.stack([item['bbox'] for item in flat_data])\n","    cropped_K = torch.stack([item['cropped_K'] for item in flat_data])\n","\n","    return {\n","        'rgb': rgb,\n","        'depth': depth,\n","        'rotation': rotation,\n","        'translation': translation,\n","        'object_id': object_ids,\n","        'norm_bbox': bbox,\n","        'cropped_K': cropped_K\n","    }"]},{"cell_type":"markdown","metadata":{"id":"ZbHqpO5TaAgq"},"source":["## Dataloaders"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2ILaA8ppaC5E"},"outputs":[],"source":["from torch.utils.data import DataLoader\n","import os\n","import numpy as np\n","\n","# Path to the full dataset.\n","dataset_root = os.path.join(full_project_path, 'dataset/LineMOD/Linemod_preprocessed/data')\n","models_root = os.path.join(full_project_path, 'dataset/LineMOD/Linemod_preprocessed/models')\n","\n","# What parts of the dataset would you like to include?\n","#folders = [1]\n","#print(f\"Loading data folder(s): {folders}\")\n","\n","# Defining the dataset splits.\n","train_dataset = PoseEstimationDataset(dataset_root,\n","                                      models_root,\n","                                      #folders=folders, # Comment out to train on the entire thing.\n","                                      split='train')\n","\n","test_dataset = PoseEstimationDataset(dataset_root,\n","                                     models_root,\n","                                     #folders =folders, # Comment out to train on the entire thing.\n","                                     split='test')\n","\n","print(f\"Training dataset size: {len(train_dataset)}\")\n","print(f\"Testing dataset size: {len(test_dataset)}\")\n","\n","# Print the list of objects present in the defined training set.\n","_, orig_ids = train_dataset.getMappedIDs()\n","print(f\"Training on {len(orig_ids)} object types: {[f'{oid:02d}' for oid in sorted(orig_ids)]}\")\n","\n","# Define dataloaders.\n","num_workers = 2\n","batch_size = 32\n","\n","train_loader = DataLoader(\n","    train_dataset,\n","    batch_size=batch_size,\n","    shuffle=True,\n","    num_workers=num_workers,\n","    pin_memory=True,\n","    collate_fn=flatten_collate_fn\n",")\n","\n","test_loader = DataLoader(\n","    test_dataset,\n","    batch_size=batch_size,\n","    shuffle=False,\n","    num_workers=num_workers,\n","    pin_memory=True,\n","    collate_fn=flatten_collate_fn\n",")\n","\n","# Storing the mapping.\n","train_dataset.save_mapping()"]},{"cell_type":"markdown","metadata":{"id":"B1_YULW7bevX"},"source":["## Functions for training and evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pyIh6C7gbjDJ"},"outputs":[],"source":["def train_model(epoch, model, train_loader, criterion, optimizer, device):\n","    \"\"\"\n","    Trains the PoseNet model for one epoch.\n","\n","    Args:\n","        epoch (int): Current training epoch number.\n","        model (nn.Module): PoseNet model to be trained.\n","        train_loader (DataLoader): DataLoader with training batches.\n","        criterion (function): Loss function (not used directly here).\n","        optimizer (torch.optim.Optimizer): Optimizer instance.\n","        device (torch.device): Target device (e.g., 'cuda').\n","\n","    Returns:\n","        nn.Module: The trained model.\n","    \"\"\"\n","    model.train()\n","    running_loss = 0.0\n","    total = 0\n","\n","    # Loop over batches with tqdm progress bar.\n","    for batch_idx, data in enumerate(tqdm(train_loader, desc=f'Epoch {epoch}', leave=False, unit=\"batch\")):\n","\n","        # Move data to device.\n","        crop_rgb = data['rgb'].to(device)\n","        crop_depth = data['depth'].to(device)\n","        t_gt = data['translation'].to(device)\n","        R_gt = data['rotation'].to(device)\n","        norm_bbox = data['norm_bbox'].to(device)\n","        cropped_K = data['cropped_K'].to(device)\n","        object_ids = data['object_id'].to(device)\n","\n","        # Forward pass through the model.\n","        t_pred, quat_pred = model(crop_rgb, crop_depth, norm_bbox, cropped_K, object_ids)\n","\n","        # Convert ground truth rotation matrix to quaternion.\n","        quat_gt = matrix_to_quaternion_batch(R_gt)\n","\n","        # Use a dynamic loss weighting schedule based on epoch.\n","        if epoch < 5:\n","            beta = 1\n","            weight_xyz = (0, 0, 0.1)\n","        elif epoch < 10:\n","            beta = 5\n","            weight_xyz = (0.5, 0.5, 0.4)\n","        else:\n","            beta = 10\n","            weight_xyz = (0.1, 0.1, 1)\n","\n","        # Compute loss using combined MSE (translation + rotation).\n","        loss, _ = computeMSE(quat_pred, t_pred, quat_gt, t_gt,\n","                             quat=True, weight_xyz=weight_xyz, beta=beta)\n","\n","        # Skip invalid loss values.\n","        if torch.isnan(loss) or torch.isinf(loss):\n","            print(f\"⚠️ Skipping batch {batch_idx} due to invalid loss (NaN or Inf)\")\n","            continue\n","\n","        # Backpropagation.\n","        optimizer.zero_grad()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Track cumulative loss.\n","        running_loss += loss.item()\n","        total += t_gt.size(0)\n","\n","    train_loss = running_loss / len(train_loader)\n","    print(f'Epoch {epoch} | Loss: {train_loss:.6f} | LR: {optimizer.param_groups[0][\"lr\"]:.6f}')\n","    return model\n","\n","def evaluate_model(model, val_loader, dataset, device, track_per_object=False):\n","    \"\"\"\n","    Evaluates the PoseNet model using ADD and loss metrics.\n","\n","    Args:\n","        model (nn.Module): Trained model to evaluate.\n","        val_loader (DataLoader): Validation dataset loader.\n","        dataset (PoseEstimationDataset): Dataset class instance (for model point loading).\n","        device (torch.device): Device to evaluate on.\n","        track_per_object (bool): If True, track ADD scores per object ID.\n","\n","    Returns:\n","        tuple: (avg_loss, avg_ADD, (x_loss, y_loss, z_loss, angular_error))\n","    \"\"\"\n","    model.eval()\n","    running_loss = 0.0\n","    add_total = 0.0\n","    num_samples = 0\n","    first_round = True  # Flag to print detailed loss only once.\n","\n","    # Load ID mapping: mapped_id → original object ID.\n","    idx_to_id = {v: k for k, v in PoseEstimationDataset.load_mapping(\"object_id_mapping.json\").items()}\n","\n","    # Initialize per-object ADD tracking.\n","    add_per_object = {orig_id: [] for orig_id in idx_to_id.values()} if track_per_object else None\n","\n","    model_points_cache = {}  # Cache to avoid reloading model points.\n","    return_tuple = tuple()\n","\n","    with torch.no_grad():\n","        for data in tqdm(val_loader, desc=\"Evaluating\", leave=False):\n","\n","            # Move inputs to device.\n","            crop_rgb = data['rgb'].to(device)\n","            crop_depth = data['depth'].to(device)\n","            t_gt = data['translation'].to(device)\n","            R_gt = data['rotation'].to(device)\n","            object_ids = data['object_id']\n","            norm_bbox = data['norm_bbox'].to(device)\n","            cropped_K = data['cropped_K'].to(device)\n","\n","            # Forward pass and pose prediction.\n","            t_pred, quat_pred = model(crop_rgb, crop_depth, norm_bbox, cropped_K, object_ids)\n","            quat_gt = matrix_to_quaternion_batch(R_gt)\n","            R_pred = quaternion_to_matrix_batch(quat_pred)\n","\n","            # Compute loss and optionally print detailed metrics once.\n","            loss, loss_tuple = computeMSE(quat_pred, t_pred, quat_gt, t_gt, quat=True, print_mse=first_round)\n","            if first_round:\n","                return_tuple = loss_tuple\n","            first_round = False\n","            running_loss += loss.item()\n","\n","            # Compute ADD metric for each object in batch.\n","            for i in range(crop_rgb.size(0)):\n","                mapped_id = int(object_ids[i])\n","                original_id = idx_to_id[mapped_id]\n","\n","                # Use cached model points if available.\n","                if original_id not in model_points_cache:\n","                    model_np = dataset.load_3D_model(original_id)\n","                    model_points_cache[original_id] = torch.tensor(model_np, dtype=torch.float32).to(device)\n","\n","                model_points = model_points_cache[original_id]\n","\n","                # ADD metric (predicted vs ground truth transformation).\n","                add = computeADD(R_pred[i], t_pred[i], R_gt[i], t_gt[i], model_points)\n","                add_total += add\n","                num_samples += 1\n","\n","                if track_per_object:\n","                    add_per_object[original_id].append(add)\n","\n","    avg_loss = running_loss / len(val_loader)\n","    avg_add = add_total / num_samples\n","    print(f'Validation Loss: {avg_loss:.6f}, Avg ADD: {avg_add:.4f}')\n","\n","    # Optionally print per-object ADD stats.\n","    if track_per_object:\n","        print(\"\\nPer-object ADD (mean):\")\n","        for obj_id, adds in sorted(add_per_object.items()):\n","            if adds:\n","                mean_add = np.mean(adds)\n","                print(f\"  Object {obj_id:02d}: ADD = {mean_add:.4f}\")\n","            else:\n","                print(f\"  Object {obj_id:02d}: No samples\")\n","\n","    return avg_loss, avg_add, return_tuple"]},{"cell_type":"markdown","metadata":{"id":"gyej-4XJ4ksF"},"source":["## Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QP6_J28o4mEH"},"outputs":[],"source":["import os\n","import torch\n","import matplotlib.pyplot as plt\n","from torch import nn, optim\n","from tqdm import tqdm\n","\n","# Flags for saving locally or to Google Drive.\n","SAVE_LOCAL = True\n","SAVE_DRIVE = True\n","\n","def train_and_evaluate(model, train_loader, test_loader, train_dataset, test_dataset,\n","                       full_project_path, num_epochs=10, patience=5, start_epoch=1):\n","    \"\"\"\n","    Trains and evaluates PoseNet over multiple epochs, with support for checkpointing,\n","    early stopping, and performance plotting.\n","\n","    Args:\n","        model (nn.Module): PoseNet_RGBD model instance.\n","        train_loader (DataLoader): Dataloader for training data.\n","        test_loader (DataLoader): Dataloader for validation/test data.\n","        train_dataset (Dataset): Training dataset instance (for diagnostics).\n","        test_dataset (Dataset): Test dataset instance (used for ADD calculation).\n","        full_project_path (str): Root directory for saving models and logs.\n","        num_epochs (int): Number of training epochs.\n","        patience (int): Early stopping patience based on ADD.\n","        start_epoch (int): Epoch to resume training from (if checkpoint exists).\n","\n","    Returns:\n","        tuple: (trained model, list of train losses, list of ADD values)\n","    \"\"\"\n","\n","    # === Path setup ===\n","    checkpoint_path_local = \"/content/checkpoint_OP.pth\"\n","    checkpoint_path_drive = os.path.join(full_project_path, \"models/checkpoint_OP.pth\")\n","    best_model_path_drive = os.path.join(full_project_path, \"models/best_posenet_OP.pt\")\n","    best_model_path_local = \"/content/best_posenet_OP.pt\"\n","\n","    # === Device configuration ===\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    print(\"<<<<<<Using GPU>>>>>>\" if torch.cuda.is_available() else \"<<<<<<Using CPU>>>>>>\")\n","    model.to(device)\n","\n","    # === Optimizer and Learning Rate Scheduler ===\n","    optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad], lr=0.001, weight_decay=0.005)\n","    lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3)\n","\n","    # === Resume from checkpoint (if it's available) ===\n","    train_losses, add_losses, ang_losses = [], [], []\n","    z_losses, x_losses, y_losses = [], [], []\n","    best_add = float('inf')\n","    if os.path.exists(checkpoint_path_local):\n","        checkpoint = torch.load(checkpoint_path_local, map_location=device)\n","        model.load_state_dict(checkpoint['model'])\n","        optimizer.load_state_dict(checkpoint['optimizer'])\n","        best_add = checkpoint['best_add']\n","        train_losses = checkpoint['train_losses']\n","        add_losses = checkpoint['add_losses']\n","        x_losses = checkpoint['x_losses']\n","        y_losses = checkpoint['y_losses']\n","        z_losses = checkpoint['z_losses']\n","        ang_losses = checkpoint['ang_losses']\n","        start_epoch = checkpoint['epoch'] + 1\n","        print(f\"Resumed training from epoch {start_epoch}.\")\n","    else:\n","        print(\"No checkpoint found, starting from epoch 1.\")\n","\n","    counter = 0  # Counter for early stopping.\n","\n","    # === Main Training Loop ===\n","    for epoch in range(start_epoch, num_epochs + 1):\n","        print(f\"\\n--------- Starting Epoch {epoch}/{num_epochs} ---------\")\n","        print(f\">>>>>>>Current best ADD is {best_add:.4f}<<<<<<<<<\")\n","        print(f\"Invalid samples found in dataset: {train_dataset.nrInvalidObjects()}\")\n","\n","        # === Training Phase ===\n","        model = train_model(epoch, model, train_loader, computeMSE, optimizer, device)\n","\n","        # === Evaluation Phase ===\n","        avg_loss, avg_add, loss_tuple = evaluate_model(model, test_loader, test_dataset, device)\n","\n","        # Recording metrics.\n","        train_losses.append(avg_loss)\n","        add_losses.append(avg_add)\n","        x_losses.append(loss_tuple[0].item())\n","        y_losses.append(loss_tuple[1].item())\n","        z_losses.append(loss_tuple[2].item())\n","        ang_losses.append(loss_tuple[3])\n","\n","        # === Best Model Saving ===\n","        if avg_add < best_add:\n","            best_add = avg_add\n","            counter = 0\n","            model.eval()\n","            if SAVE_LOCAL:\n","                torch.save(model.state_dict(), best_model_path_local)\n","                print(f\"✅ New best ADD: {avg_add:.4f} (saved model locally)\")\n","            if SAVE_DRIVE:\n","                torch.save(model.state_dict(), best_model_path_drive)\n","                print(f\"☁️ New best ADD: {avg_add:.4f} (saved model on Google Drive)\")\n","        else:\n","            counter += 1\n","            if counter >= patience:\n","                print(\"⏹ Early stopping triggered.\")\n","                break\n","\n","        # === Save Checkpoint (regardless of improvement) ===\n","        checkpoint = {\n","            'epoch': epoch,\n","            'model': model.state_dict(),\n","            'optimizer': optimizer.state_dict(),\n","            'best_add': best_add,\n","            'train_losses': train_losses,\n","            'add_losses': add_losses,\n","            'x_losses': x_losses,\n","            'y_losses': y_losses,\n","            'z_losses': z_losses,\n","            'ang_losses': ang_losses\n","        }\n","        if SAVE_LOCAL:\n","            torch.save(checkpoint, checkpoint_path_local)\n","            print(\"💾 Checkpoint saved locally.\")\n","        if SAVE_DRIVE:\n","            torch.save(checkpoint, checkpoint_path_drive)\n","            print(\"☁️ Checkpoint saved on Google Drive.\")\n","\n","        # Update learning rate based on validation loss.\n","        lr_scheduler.step(avg_loss)\n","        print(f\"Epoch {epoch}/{num_epochs} | Average ADD: {avg_add:.4f}\")\n","\n","    # === Plotting Training Curve after finished training (Loss and ADD) ===\n","    epochs_run = list(range(1, len(train_losses) + 1))\n","\n","    fig, ax1 = plt.subplots()\n","    ax2 = ax1.twinx()\n","    ax1.plot(epochs_run, train_losses, 'g-', label='MSE Loss')\n","    ax2.plot(epochs_run, add_losses, 'b-', label='ADD')\n","    ax1.set_xlabel('Epoch')\n","    ax1.set_ylabel('MSE Loss', color='g')\n","    ax2.set_ylabel('ADD (m)', color='b')\n","    ax1.set_title(\"Training Loss and Validation ADD\")\n","    ax1.grid(True)\n","    lines1, labels1 = ax1.get_legend_handles_labels()\n","    lines2, labels2 = ax2.get_legend_handles_labels()\n","    ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n","    plt.tight_layout()\n","    plt.show()\n","\n","    # === Plotting Translation and Angular Error ===\n","    fig, ax1 = plt.subplots()\n","    ax1.plot(epochs_run, x_losses, label='X loss', color='tab:red')\n","    ax1.plot(epochs_run, y_losses, label='Y loss', color='tab:green')\n","    ax1.plot(epochs_run, z_losses, label='Z loss', color='tab:blue')\n","    ax1.set_ylabel('Translation Loss (MSE)', color='black')\n","    ax1.set_xlabel('Epoch')\n","    ax1.grid(True)\n","\n","    ax2 = ax1.twinx()\n","    ax2.plot(epochs_run, ang_losses, label='Angular Error (deg)', color='tab:purple', linestyle='--')\n","    ax2.set_ylabel('Angular Error (°)', color='tab:purple')\n","    ax2.tick_params(axis='y', labelcolor='tab:purple')\n","\n","    lines1, labels1 = ax1.get_legend_handles_labels()\n","    lines2, labels2 = ax2.get_legend_handles_labels()\n","    ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n","\n","    plt.title(\"Translation Losses and Angular Error Over Epochs\")\n","    plt.tight_layout()\n","    plt.show()\n","\n","    return model, train_losses, add_losses\n","\n","# === Initialize model ===\n","_, num_objects = train_dataset.getMappedIDs()\n","num_obj = len(num_objects)\n","print(f\"This model will be trained to find {num_obj} object(s).\")\n","\n","model = PoseNet_RGBD(\n","    num_objects=num_obj,\n","    embedding_dim=16,\n","    img_size=(224, 224)\n",")\n","\n","# === Train and Evaluate ===\n","_ = train_and_evaluate(\n","    model=model,\n","    train_loader=train_loader,\n","    test_loader=test_loader,\n","    train_dataset=train_dataset,\n","    test_dataset=test_dataset,\n","    full_project_path=full_project_path,\n","    num_epochs=65,\n","    patience=15\n",")"]},{"cell_type":"markdown","metadata":{"id":"6_toHUHZQ1hg"},"source":["## Functions for plotting the results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W-_xRYM5Q237"},"outputs":[],"source":["import os\n","import cv2\n","import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torchvision.utils import save_image\n","\n","def draw_model_projection(image, points, color, radius=1):\n","    \"\"\"\n","    Draws projected 2D model points as circles on the image.\n","\n","    Args:\n","        image (np.ndarray): BGR image.\n","        points (np.ndarray): 2D projected points (N, 2).\n","        color (tuple): BGR color (e.g., (0, 255, 0)).\n","        radius (int): Radius of the circle to draw.\n","    \"\"\"\n","    for pt in points.astype(int):\n","        x, y = pt\n","        if 0 <= x < image.shape[1] and 0 <= y < image.shape[0]:\n","            cv2.circle(image, (x, y), radius, color, -1)\n","\n","def draw_legend(image, labels_colors):\n","    \"\"\"\n","    Draws a simple color legend (e.g., \"GT\" vs \"Pred\") on the image.\n","\n","    Args:\n","        image (np.ndarray): BGR image.\n","        labels_colors (list): List of (label, color) pairs.\n","    \"\"\"\n","    x, y, spacing = 10, 25, 25\n","    font_scale = 0.4\n","    text_thickness = 1\n","    box_width = 10\n","    box_height = 10\n","    for i, (label, color) in enumerate(labels_colors):\n","        cv2.putText(image, label, (x + box_width + 5, y + i * spacing),\n","                    cv2.FONT_HERSHEY_SIMPLEX, font_scale, color, text_thickness)\n","        cv2.rectangle(image,\n","                      (x, y - box_height // 2 + i * spacing),\n","                      (x + box_width, y + box_height // 2 + i * spacing),\n","                      color, -1)\n","\n","def draw_axes(image, R, t, K, is_gt=False, axis_length=0.05, thickness=2):\n","    \"\"\"\n","    Projects and draws coordinate axes (X, Y, Z) for a given pose.\n","\n","    Args:\n","        image (np.ndarray): BGR image.\n","        R (np.ndarray): 3x3 rotation matrix.\n","        t (np.ndarray): 3D translation vector.\n","        K (np.ndarray): 3x3 camera intrinsic matrix.\n","        is_gt (bool): Whether this is ground truth (duller colors).\n","        axis_length (float): Length of axes in meters.\n","        thickness (int): Line thickness.\n","\n","    Returns:\n","        np.ndarray: Annotated image.\n","    \"\"\"\n","    axes_3d = np.array([[0, 0, 0],\n","                        [axis_length, 0, 0],\n","                        [0, axis_length, 0],\n","                        [0, 0, axis_length]], dtype=np.float32)\n","\n","    pts_2d = (K @ (axes_3d @ R.T + t).T).T\n","    pts_2d = pts_2d[:, :2] / pts_2d[:, 2:3]\n","    pts_2d = pts_2d.astype(int)\n","\n","    origin = tuple(pts_2d[0])\n","    cv2.circle(image, origin, 6, (255, 255, 255), -1)\n","\n","    color_map = [(0, 0, 255), (0, 255, 0), (255, 0, 0)] if not is_gt else \\\n","                [(100, 100, 255), (100, 255, 100), (255, 100, 100)]\n","\n","    for i in range(1, 4):\n","        cv2.line(image, origin, tuple(pts_2d[i]), color_map[i - 1], thickness)\n","\n","    return image\n","\n","def project(pts, R, t, K):\n","    \"\"\"\n","    Projects 3D model points into 2D image space.\n","\n","    Args:\n","        pts (np.ndarray): 3D model points (N, 3).\n","        R (np.ndarray): 3x3 rotation matrix.\n","        t (np.ndarray): 3D translation vector.\n","        K (np.ndarray): 3x3 camera intrinsics.\n","\n","    Returns:\n","        np.ndarray: 2D projected points (N, 2).\n","    \"\"\"\n","    proj = (K @ (pts @ R.T + t).T).T\n","    return proj[:, :2] / proj[:, 2:3]\n","\n","def visualize_pose_prediction(obj, model, dataset, obj_id, device,\n","                              draw_axes_flag=False, draw_models=True, use_full_frame=True,\n","                              save_dir=None, save_prefix=\"result\", legend=False):\n","    \"\"\"\n","    Visualizes predicted vs ground truth pose for a given object crop.\n","\n","    Args:\n","        obj (dict): Single object crop entry from dataset.\n","        model (nn.Module): PoseNet model.\n","        dataset (PoseEstimationDataset): Dataset instance (for 3D model lookup).\n","        obj_id (int): Mapped object ID (internal index).\n","        device (torch.device): Device to run inference on.\n","        draw_axes_flag (bool): Whether to draw 3D coordinate axes.\n","        draw_models (bool): Whether to draw full model projections.\n","        use_full_frame (bool): Whether to also show full frame projection.\n","        save_dir (str): Directory to save visualizations (optional).\n","        save_prefix (str): Filename prefix for saved files.\n","        legend (bool): Whether to draw a color legend.\n","    \"\"\"\n","    # Prepare the model inputs.\n","    rgb = obj['cropped_rgb'].unsqueeze(0).to(device)\n","    crop_depth = obj['cropped_depth'].unsqueeze(0).to(device)\n","    norm_bbox = obj['norm_bbox'].unsqueeze(0).to(device)\n","    cropped_K = obj['cropped_K'].unsqueeze(0).to(device)\n","    obj_tensor = torch.tensor([obj_id], dtype=torch.long).to(device)\n","\n","    R_gt = obj['rotation']\n","    t_gt = obj['translation']\n","\n","    # Inference.\n","    with torch.no_grad():\n","        t_pred, quat_pred = model(rgb, crop_depth, norm_bbox, cropped_K, obj_tensor)\n","\n","    # Convert predictions to the correct format.\n","    R_pred = quaternion_to_matrix_batch(quat_pred.detach()).squeeze().cpu().numpy()\n","    t_pred = t_pred.detach().squeeze().cpu().numpy()\n","\n","    quat_gt = matrix_to_quaternion_batch(torch.tensor(R_gt, dtype=torch.float32).to(device))\n","    with torch.no_grad():\n","        ang_err = quaternion_angular_error(quat_pred, quat_gt).item()\n","\n","    # Load 3D model.\n","    original_id = dataset.idx_to_id[obj_id]\n","    model_points = dataset.load_3D_model(original_id)\n","\n","    # === Visualize Cropped Image ===\n","    crop_rgb = obj['cropped_rgb'].cpu().numpy().transpose(1, 2, 0)\n","    crop_rgb = ((crop_rgb * 0.229 + 0.485).clip(0, 1) * 255).astype(np.uint8)\n","    crop_rgb = cv2.cvtColor(crop_rgb, cv2.COLOR_RGB2BGR)\n","    cropped_K_np = cropped_K.squeeze().cpu().numpy()\n","    vis_crop = crop_rgb.copy()\n","\n","    if draw_axes_flag:\n","        vis_crop = draw_axes(vis_crop, R_pred, t_pred, cropped_K_np)\n","        vis_crop = draw_axes(vis_crop, R_gt, t_gt, cropped_K_np, is_gt=True)\n","\n","    if draw_models:\n","        proj_gt = project(model_points, R_gt, t_gt, cropped_K_np)\n","        proj_pred = project(model_points, R_pred, t_pred, cropped_K_np)\n","        draw_model_projection(vis_crop, proj_gt, (0, 255, 0))\n","        draw_model_projection(vis_crop, proj_pred, (0, 0, 255))\n","\n","    if legend:\n","        draw_legend(vis_crop, [(\"GT\", (0, 255, 0)), (\"Pred\", (0, 0, 255))])\n","\n","    # Computing the ADD metric.\n","    add = computeADD(R_pred, t_pred, R_gt, t_gt, model_points)\n","    print(f\"\\n➡️ Mapped object ID {obj_id:02d}\")\n","    print(f\"ADD: {add:.4f} m | Angular Error: {ang_err:.2f}°\")\n","\n","    plt.figure()\n","    plt.imshow(cv2.cvtColor(vis_crop, cv2.COLOR_BGR2RGB))\n","    plt.title(f\"[Cropped] Obj {original_id:02d}\")\n","    plt.axis(\"off\")\n","    plt.show()\n","\n","    if save_dir:\n","        os.makedirs(save_dir, exist_ok=True)\n","        save_path = os.path.join(save_dir, f\"{save_prefix}_cropped_obj{obj_id:02d}.png\")\n","        cv2.imwrite(save_path, vis_crop)\n","\n","    # === Visualize Full Image View ===\n","    if use_full_frame:\n","        full_image = cv2.cvtColor(np.array(obj['original_rgb']), cv2.COLOR_RGB2BGR)\n","        original_K = obj['original_K'].numpy()\n","\n","        if draw_axes_flag:\n","            full_image = draw_axes(full_image, R_pred, t_pred, original_K)\n","            full_image = draw_axes(full_image, R_gt, t_gt, original_K, is_gt=True)\n","\n","        if draw_models:\n","            proj_gt_full = project(model_points, R_gt, t_gt, original_K)\n","            proj_pred_full = project(model_points, R_pred, t_pred, original_K)\n","            draw_model_projection(full_image, proj_gt_full, (0, 255, 0))\n","            draw_model_projection(full_image, proj_pred_full, (0, 0, 255))\n","\n","        if legend:\n","            draw_legend(full_image, [(\"GT\", (0, 255, 0)), (\"Pred\", (0, 0, 255))])\n","\n","        plt.figure()\n","        plt.imshow(cv2.cvtColor(full_image, cv2.COLOR_BGR2RGB))\n","        plt.title(f\"[Full Frame] Obj {original_id:02d}\")\n","        plt.axis(\"off\")\n","        plt.show()\n","\n","        if save_dir:\n","            save_path = os.path.join(save_dir, f\"{save_prefix}_fullframe_obj{obj_id:02d}.png\")\n","            cv2.imwrite(save_path, full_image)\n","\n","def run_visualization(model, dataset, device, target_obj_ids,\n","                      img_idx=0, save_dir=None, draw_axes=False,\n","                      draw_legend=False, draw_models=False):\n","    \"\"\"\n","    Runs pose prediction visualization for selected objects in a given image.\n","\n","    Args:\n","        model (nn.Module): Trained PoseNet model.\n","        dataset (Dataset): PoseEstimationDataset instance.\n","        device (torch.device): CUDA or CPU device.\n","        target_obj_ids (set): Set of internal object IDs to visualize.\n","        img_idx (int): Index of the sample image to visualize.\n","        save_dir (str): Directory to save images (optional).\n","        draw_axes (bool): Whether to draw 3D coordinate axes.\n","        draw_legend (bool): Whether to draw GT/Pred legend.\n","        draw_models (bool): Whether to project and overlay 3D models.\n","    \"\"\"\n","    model.eval()\n","    idx_to_id = {v: k for k, v in dataset.id_to_idx.items()}\n","\n","    with torch.no_grad():\n","        data_item = dataset[img_idx]\n","        found_obj_ids = set()\n","\n","        original_rgb = data_item['original_rgb']\n","        original_K = data_item['original_K']\n","\n","        for obj in data_item['objects']:\n","            obj_id = obj['object_id']\n","            if obj_id in target_obj_ids:\n","                found_obj_ids.add(obj_id)\n","\n","                # Inject full image and intrinsics into the object for visualization.\n","                obj['original_rgb'] = original_rgb\n","                obj['original_K'] = original_K\n","\n","                visualize_pose_prediction(\n","                    obj, model, dataset, obj_id, device,\n","                    draw_axes_flag=draw_axes,\n","                    draw_models=draw_models,\n","                    legend=draw_legend,\n","                    save_dir=save_dir,\n","                    save_prefix=f\"img{img_idx:03d}_obj{obj_id:02d}\"\n","                )\n","\n","        # Warn if some requested objects weren't present in the image.\n","        missing_ids = set(target_obj_ids) - found_obj_ids\n","        if missing_ids:\n","            missing_original_ids = sorted([idx_to_id[mapped_id] for mapped_id in missing_ids])\n","            print(f\"⚠️ Note: These object IDs (mapped) were not present in image {img_idx}: {sorted(missing_ids)}\")"]},{"cell_type":"markdown","metadata":{"id":"2nz2cPzV29bo"},"source":["## Plotting the results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UrmjxppEp4H2"},"outputs":[],"source":["# --- Dataset & Paths ---\n","dataset_root = os.path.join(full_project_path, 'dataset/LineMOD/Linemod_preprocessed/data')\n","models_root = os.path.join(full_project_path, \"dataset/LineMOD/Linemod_preprocessed/models\")\n","\n","# Load a subset (e.g., objects 1 and 13)\n","folders = [2] # List of folder [1,2,3].\n","dataset = PoseEstimationDataset(dataset_root,\n","                                models_root,\n","                                folders=folders)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V47LjZSf3AXX"},"outputs":[],"source":["# Print the available object IDs from dataset.\n","print(\"Object IDs in dataset:\", dataset.object_ids)  # Original LineMOD object IDs.\n","all_mapped_ids, _ = dataset.getMappedIDs()\n","print(\"Mapped IDs:\", all_mapped_ids)  # Internally mapped IDs used in training/evaluation\n","\n","# === Select which original object IDs to visualize ===\n","original_ids_to_visualize = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n","# original_ids_to_visualize = [13]  # If you want to test a single object.\n","\n","# === Load Trained Model ===\n","num_objects = len(original_ids_to_visualize)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Initialize model with same number of object classes used during training.\n","model = PoseNet_RGBD(num_objects=13)\n","\n","# Load pretrained model weights\n","model.load_state_dict(torch.load('/content/best_posenet_OP.pt', map_location=device))\n","model.to(device)\n","\n","# === Prepare Visualization Parameters ===\n","# Convert original LineMOD IDs to internal mapped IDs.\n","mapped_ids, _ = dataset.getMappedIDs(original_ids_to_visualize)\n","\n","img_idx = 20 # Datasets index of the image to visualize.\n","save_dir = \"/content/visualizations\"  # Folder to save visualization images.\n","\n","# === Run Visualization ===\n","run_visualization(\n","    model=model,\n","    dataset=dataset,\n","    device=device,\n","    target_obj_ids=mapped_ids,  # Mapped object IDs to look for in the selected image.\n","    img_idx=img_idx,\n","    save_dir=save_dir,\n","    draw_axes=True,     # Draw 3D coordinate axes (Pred vs GT).\n","    draw_legend=False,  # Hide the GT/Pred legend.\n","    draw_models=False   # Visualize 3D model for GT (green) and pred (red).\n",")"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["Vu8EdO6H3r5T","q9iQ9GE_3wQG","KPyo8_eyCgyr","87dUPITNvUGI","keS9PBYASWjI","ZbHqpO5TaAgq","B1_YULW7bevX","gyej-4XJ4ksF","6_toHUHZQ1hg","2nz2cPzV29bo"],"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyNKXMOcNtE94APy6lOg3w3M"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}