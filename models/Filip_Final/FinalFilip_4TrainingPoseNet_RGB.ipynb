{"cells":[{"cell_type":"markdown","metadata":{"id":"Vu8EdO6H3r5T"},"source":["## Downloading packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-D9ztHeLblQv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747484137335,"user_tz":-120,"elapsed":12126,"user":{"displayName":"Filip Nykvist","userId":"11309094825996781814"}},"outputId":"a4e45b7f-4126-44c0-e888-38ae583e38e5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: trimesh in /usr/local/lib/python3.11/dist-packages (4.6.9)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.11/dist-packages (from trimesh) (2.0.2)\n","Requirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (8.1.7)\n","Requirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (0.2.2)\n","Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (7.34.0)\n","Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (5.7.1)\n","Requirement already satisfied: widgetsnbextension~=4.0.14 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (4.0.14)\n","Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (3.0.15)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (75.2.0)\n","Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n","Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.13)\n"]}],"source":["!pip install trimesh\n","!pip install --upgrade ipywidgets"]},{"cell_type":"markdown","metadata":{"id":"q9iQ9GE_3wQG"},"source":["## Mounting to Google Drive"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4574,"status":"ok","timestamp":1747484141914,"user":{"displayName":"Filip Nykvist","userId":"11309094825996781814"},"user_tz":-120},"id":"xeacSnj630HZ","outputId":"95247fef-6c30-4cfb-d7fa-01e7b2ca3c84"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["import os\n","from google.colab import drive\n","\n","# Define the base path for your Google Drive.\n","base_path = '/content/drive'\n","\n","# Define the specific folder path within your Google Drive.\n","folder_path = 'MyDrive/Colab Notebooks/Machine Learning and Deep Learning/Project'\n","\n","# Combine the base path and folder path to create the full mount path.\n","full_project_path = os.path.join(base_path, folder_path)\n","\n","# Mount your drive.\n","drive.mount(base_path, force_remount=True)"]},{"cell_type":"markdown","metadata":{"id":"KPyo8_eyCgyr"},"source":["## Creating custom dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"68wVlpfdCjJi"},"outputs":[],"source":["# Standard libraries.\n","import os\n","import yaml\n","import numpy as np\n","import json\n","\n","# PyTorch and image processing.\n","import torch\n","import trimesh\n","from torch.utils.data import Dataset\n","from torchvision import transforms\n","from sklearn.model_selection import train_test_split\n","from PIL import Image\n","\n","class PoseEstimationDataset(Dataset):\n","    \"\"\"\n","    Custom PyTorch dataset for loading RGB images, camera intrinsics,\n","    3D object models, and 6D pose annotations from the LineMOD dataset.\n","    \"\"\"\n","\n","    def __init__(self, dataset_root,\n","                 models_root,\n","                 folders=list(range(1, 16)),\n","                 split='train',\n","                 train_ratio=0.8,\n","                 seed=42,\n","                 img_size=(224, 224)):\n","        \"\"\"\n","        Initializes the dataset by reading object annotations, images,\n","        and preparing train/test splits.\n","\n","        Args:\n","            dataset_root (str): Root path to LineMOD dataset.\n","            models_root (str): Path to 3D object models and models_info.yml.\n","            folders (list): List of object IDs (integers from 1 to 15).\n","            split (str): Either 'train' or 'test'.\n","            train_ratio (float): Proportion of data used for training.\n","            seed (int): Random seed for reproducibility.\n","            img_size (tuple): Target image size for input to neural network.\n","        \"\"\"\n","        self.dataset_root = dataset_root\n","        self.models_root = models_root\n","        self.split = split\n","        self.train_ratio = train_ratio\n","        self.seed = seed\n","        self.img_size = img_size\n","        self.models = {}  # Cache for loaded 3D models\n","        self.invalid_entries = 0\n","\n","        # Load object-specific metadata (diameter, size, etc.).\n","        self.models_info_path = os.path.join(models_root, 'models_info.yml')\n","        with open(self.models_info_path, 'r') as f:\n","            self.models_info = yaml.safe_load(f)\n","\n","        # Cache ground truth and camera info per object.\n","        self.gt_data = {}\n","        self.info_data = {}\n","        self.all_samples = []  # List of (object_id, sample_id) tuples.\n","\n","        for obj_id in folders:\n","            obj_folder = os.path.join(dataset_root, f\"{obj_id:02d}\")\n","            gt_path = os.path.join(obj_folder, 'gt.yml')\n","            info_path = os.path.join(obj_folder, 'info.yml')\n","\n","            if not os.path.exists(gt_path):\n","                continue\n","\n","            # Load ground truth and camera intrinsic data.\n","            with open(gt_path, 'r') as f:\n","                gt = yaml.safe_load(f)\n","            with open(info_path, 'r') as f:\n","                info = yaml.safe_load(f)\n","\n","            self.gt_data[obj_id] = gt\n","            self.info_data[obj_id] = info\n","\n","            for sample_id in gt.keys():\n","                self.all_samples.append((obj_id, int(sample_id)))\n","\n","        # Map original object IDs to index values with start 0 and incrementing order.\n","        object_id_set = set()\n","        for obj_id, sample_id in self.all_samples:\n","            for ann in self.gt_data[obj_id][sample_id]:\n","                object_id_set.add(ann['obj_id'])\n","\n","        self.object_ids = sorted(object_id_set)\n","        self.id_to_idx = {obj_id: i for i, obj_id in enumerate(self.object_ids)}\n","        self.idx_to_id = {i: obj_id for obj_id, i in self.id_to_idx.items()}\n","\n","        if not self.all_samples:\n","            raise ValueError(f\"No samples found in {dataset_root}. Check dataset structure.\")\n","\n","        # Split into train/test subsets.\n","        train, test = train_test_split(self.all_samples, train_size=self.train_ratio, random_state=self.seed)\n","        self.samples = train if self.split == 'train' else test\n","\n","        # Image preprocessing pipeline.\n","        self.transform = transforms.Compose([\n","            transforms.Resize(self.img_size),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                 std=[0.229, 0.224, 0.225])\n","        ])\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","    def save_mapping(self, filepath=None):\n","        \"\"\"Saves object ID to index mapping to a JSON file.\"\"\"\n","        if filepath is None:\n","            filepath = \"object_id_mapping.json\"\n","        with open(filepath, \"w\") as f:\n","            json.dump(self.id_to_idx, f)\n","\n","    @staticmethod\n","    def load_mapping(filepath=None):\n","        \"\"\"Loads a previously saved object ID mapping from file.\"\"\"\n","        if filepath is None:\n","            filepath = \"object_id_mapping.json\"\n","        with open(filepath, \"r\") as f:\n","            mapping = json.load(f)\n","        return {int(k): v for k, v in mapping.items()}\n","\n","    def printIDMapping(self):\n","        \"\"\"Prints object ID mapping from original ID to training index.\"\"\"\n","        print(\"Object ID Mapping (Original ‚Üí Mapped):\")\n","        for orig_id in self.object_ids:\n","            print(f\"  {orig_id:02d} ‚Üí {self.id_to_idx[orig_id]}\")\n","\n","    def nrInvalidObjects(self):\n","        \"\"\"Returns number of invalid bounding boxes encountered during cropping.\"\"\"\n","        return self.invalid_entries\n","\n","    def getMappedIDs(self, ids=None):\n","        \"\"\"\n","        Maps original object IDs to their training indices.\n","\n","        Args:\n","            ids (list or None): Original object IDs. If None, returns all mappings.\n","        Returns:\n","            mapped_ids (list): Mapped training indices.\n","            orig_ids (list): Corresponding original object IDs.\n","        \"\"\"\n","        mapped_ids = []\n","        orig_ids = []\n","        if ids is None:\n","            for orig_id in self.object_ids:\n","                mapped_ids.append(self.id_to_idx[orig_id])\n","                orig_ids.append(orig_id)\n","        else:\n","            for id in ids:\n","                if id in self.id_to_idx:\n","                    mapped_ids.append(self.id_to_idx[id])\n","                    orig_ids.append(id)\n","                else:\n","                    print(f\"‚ö†Ô∏è Warning: Object ID {id} not found in dataset and will be ignored.\")\n","        return mapped_ids, orig_ids\n","\n","    def get_model_info(self, object_id):\n","        \"\"\"Returns metadata (ex. diameter) for the specified object ID.\"\"\"\n","        if object_id not in self.models_info:\n","            raise ValueError(f\"Object ID {object_id} not in models_info.yml\")\n","        return self.models_info[object_id]\n","\n","    def load_3D_model(self, object_id):\n","        \"\"\"\n","        Loads and returns the 3D model (in meters) of the specified object.\n","        Caches the result to avoid redundant loading.\n","        \"\"\"\n","        if object_id in self.models:\n","            return self.models[object_id]\n","\n","        model_path = os.path.join(self.models_root, f\"obj_{object_id:02d}.ply\")\n","        mesh = trimesh.load(model_path)\n","        points = mesh.vertices.astype(np.float32) / 1000.0  # mm ‚Üí m\n","        self.models[object_id] = points\n","        return points\n","\n","    def cropImages(self, image, annotations, cam_K):\n","        \"\"\"\n","        Crops object regions from the image based on bounding boxes and adjusts the intrinsics.\n","\n","        Args:\n","            image (PIL.Image): Original RGB image.\n","            annotations (list): Object annotations from gt.yml.\n","            cam_K (torch.Tensor): Original 3x3 camera intrinsic matrix.\n","\n","        Returns:\n","            crop_entries (list): List of cropped image entries with adjusted intrinsics.\n","            cam_K (torch.Tensor): Original intrinsic matrix.\n","        \"\"\"\n","        crop_entries = []\n","        width, height = image.size\n","\n","        for ann in annotations:\n","            x, y, w, h = ann['obj_bb']\n","            x1 = max(0, x)\n","            y1 = max(0, y)\n","            x2 = min(width, x + w)\n","            y2 = min(height, y + h)\n","\n","            # Skip invalid bounding boxes\n","            if x2 <= x1 or y2 <= y1:\n","                self.invalid_entries += 1\n","                continue\n","\n","            cropped = image.crop((x1, y1, x2, y2))\n","            original_crop_width, original_crop_height = cropped.size\n","\n","            # Adjust intrinsics for crop and resize\n","            cropped_K = cam_K.clone()\n","            cropped_K[0, 2] -= x1\n","            cropped_K[1, 2] -= y1\n","\n","            cropped = cropped.resize(self.img_size, Image.BILINEAR)\n","            scale_x = self.img_size[0] / original_crop_width\n","            scale_y = self.img_size[1] / original_crop_height\n","            cropped_K[0, 0] *= scale_x\n","            cropped_K[0, 2] *= scale_x\n","            cropped_K[1, 1] *= scale_y\n","            cropped_K[1, 2] *= scale_y\n","\n","            cropped_rgb_tensor = self.transform(cropped)\n","            R_mat = np.array(ann['cam_R_m2c'], dtype=np.float32).reshape(3, 3)\n","            t_vec = np.array(ann['cam_t_m2c'], dtype=np.float32) / 1000.0  # mm ‚Üí m\n","\n","            # Normalized bounding box (for optional loss regularization)\n","            norm_bbox = torch.tensor([\n","                x1 / width,\n","                y1 / height,\n","                (x2 - x1) / width,\n","                (y2 - y1) / height\n","            ], dtype=torch.float32)\n","\n","            crop_entries.append({\n","                'cropped_rgb': cropped_rgb_tensor,\n","                'cropped_K': cropped_K,\n","                'object_id': ann['obj_id'],\n","                'bbox': ann['obj_bb'],\n","                'norm_bbox': norm_bbox,\n","                'rotation': R_mat,\n","                'translation': t_vec\n","            })\n","\n","        return crop_entries, cam_K\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        Loads a data sample consisting of:\n","        - Original RGB image.\n","        - Camera intrinsics.\n","        - List of cropped objects with adjusted intrinsics and pose labels.\n","        \"\"\"\n","        object_id, sample_id = self.samples[idx]\n","        folder = os.path.join(self.dataset_root, f\"{object_id:02d}\")\n","\n","        rgb_path = os.path.join(folder, 'rgb', f\"{sample_id:04d}.png\")\n","        if not os.path.exists(rgb_path):\n","            raise FileNotFoundError(f\"RGB image not found: {rgb_path}\")\n","        rgb = Image.open(rgb_path).convert(\"RGB\")\n","\n","        annotations = self.gt_data[object_id][sample_id]\n","        cam_K = torch.tensor(np.array(self.info_data[object_id][sample_id]['cam_K']).reshape(3, 3), dtype=torch.float32)\n","\n","        crop_entries, org_K = self.cropImages(rgb, annotations, cam_K)\n","\n","        # Remap object IDs from original to training indices\n","        for entry in crop_entries:\n","            true_obj_id = entry['object_id']\n","            if true_obj_id not in self.id_to_idx:\n","                raise ValueError(f\"Object ID {true_obj_id} not found in id_to_idx mapping.\")\n","            entry['object_id'] = self.id_to_idx[true_obj_id]\n","\n","        return {\n","            'sample_id': sample_id,\n","            'original_rgb': rgb,\n","            'original_K': org_K,\n","            'objects': crop_entries\n","        }"]},{"cell_type":"markdown","source":["## PoseNet6D using ResNet18"],"metadata":{"id":"87dUPITNvUGI"}},{"cell_type":"code","source":["from torch import nn\n","import torch\n","import torch.nn.functional as F\n","# Loading the resnet18 model together with pre-trained weights.\n","from torchvision.models import resnet18, ResNet18_Weights\n","\n","\n","class PoseNet6D(nn.Module):\n","    \"\"\"\n","    PoseNet6D estimates 6D object pose (translation and rotation) from cropped RGB images,\n","    using a CNN backbone (ResNet18), bounding box information, and object ID embeddings.\n","    \"\"\"\n","    def __init__(self, num_objects,\n","                 embedding_dim=16,\n","                 img_size=(224, 224),\n","                 weights=ResNet18_Weights.DEFAULT):\n","        \"\"\"\n","        Initializes the network components.\n","\n","        Args:\n","            num_objects (int): Number of unique object classes.\n","            embedding_dim (int): Size of the object ID embedding vector.\n","            img_size (tuple): Size of the input images (width, height).\n","            weights (ResNet18_Weights): Pretrained weights for ResNet18 backbone.\n","        \"\"\"\n","        super(PoseNet6D, self).__init__()\n","        self.img_size = img_size\n","\n","        # Load pretrained ResNet18 and remove the last two layers (avgpool and FC).\n","        resnet = resnet18(weights=weights)\n","        self.backbone = nn.Sequential(*list(resnet.children())[:-2])  # Output: (B, 512, 7, 7).\n","\n","        # Global average pooling to reduce spatial features to vector.\n","        self.global_pool = nn.AdaptiveAvgPool2d(1)\n","\n","        # Object ID embedding layer.\n","        self.obj_embedding = nn.Embedding(num_embeddings=num_objects,\n","                                          embedding_dim=embedding_dim)\n","\n","        # Concatenated feature size: image (512) + bbox (4) + object embedding.\n","        fused_dim = 512 + 4 + embedding_dim\n","\n","        # Fully connected layers for depth estimation (1 output value).\n","        self.fc_depth = nn.Sequential(\n","            nn.Linear(fused_dim, 256),\n","            nn.BatchNorm1d(256),\n","            nn.ReLU(),\n","            nn.Linear(256, 1)\n","        )\n","\n","        # Fully connected layers for quaternion rotation (4D unit vector).\n","        self.fc_rotation = nn.Sequential(\n","            nn.Linear(fused_dim, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 4)\n","        )\n","\n","    def forward(self, x_img, norm_bbox, K_crop, object_id):\n","        \"\"\"\n","        Forward pass to estimate translation and rotation.\n","\n","        Args:\n","            x_img (Tensor): Input cropped RGB image (B, 3, H, W).\n","            norm_bbox (Tensor): Normalized bounding boxes (B, 4).\n","            K_crop (Tensor): Intrinsics matrix adjusted to crop (B, 3, 3).\n","            object_id (Tensor): Object class indices (B,).\n","\n","        Returns:\n","            translation (Tensor): Estimated 3D translation vector (B, 3).\n","            quat (Tensor): Estimated rotation quaternion (B, 4).\n","        \"\"\"\n","        B = x_img.shape[0]\n","\n","        # CNN feature extraction.\n","        x = self.backbone(x_img)                # Shape: (B, 512, 7, 7)\n","        x = self.global_pool(x).view(B, -1)     # Shape: (B, 512)\n","\n","        # Ensure object IDs are on the correct device.\n","        object_id = object_id.to(self.obj_embedding.weight.device)\n","\n","        # Object embedding lookup.\n","        obj_feat = self.obj_embedding(object_id)  # Shape: (B, embedding_dim)\n","\n","        # Concatenate image features, bbox, and object embedding.\n","        x = torch.cat([x, norm_bbox, obj_feat], dim=1)  # Shape: (B, fused_dim)\n","\n","        # Predict object depth (z).\n","        depth = self.fc_depth(x).squeeze(1)  # Shape: (B,)\n","        depth = torch.clamp(depth, min=0.1, max=1.5)  # Clamp to avoid extreme values\n","\n","        # Predict object orientation as a unit quaternion.\n","        quat = F.normalize(self.fc_rotation(x), dim=1)  # Shape: (B, 4)\n","\n","        # Deproject normalized bbox center to camera coordinates using intrinsics.\n","        fx = K_crop[:, 0, 0]\n","        fy = K_crop[:, 1, 1]\n","        cx_crop = K_crop[:, 0, 2]\n","        cy_crop = K_crop[:, 1, 2]\n","\n","        img_w, img_h = self.img_size\n","        u = (norm_bbox[:, 0] + 0.5 * norm_bbox[:, 2]) * img_w  # Center x\n","        v = (norm_bbox[:, 1] + 0.5 * norm_bbox[:, 3]) * img_h  # Center y\n","\n","        # Back-project to 3D camera coordinates.\n","        x_cam = (u - cx_crop) * depth / fx\n","        y_cam = (v - cy_crop) * depth / fy\n","        z_cam = depth\n","\n","        translation = torch.stack([x_cam, y_cam, z_cam], dim=1)  # Shape: (B, 3)\n","        return translation, quat"],"metadata":{"id":"tov6Nf4mvZdj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"keS9PBYASWjI"},"source":["## Helper functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9zPtmURiSYNs"},"outputs":[],"source":["import torch.nn.functional as F\n","from scipy.spatial.transform import Rotation as R\n","from scipy.spatial import cKDTree\n","import numpy as np\n","import torch\n","\n","# -----------------------------\n","# ADD Metric (Average Distance)\n","# -----------------------------\n","def computeADD(R_pred, t_pred, R_gt, t_gt, model_points):\n","    \"\"\"\n","    Computes the ADD (Average Distance of Model Points) metric.\n","    Measures the mean distance between predicted and ground-truth transformed model points.\n","\n","    Args:\n","        R_pred (ndarray or Tensor): Predicted rotation matrix (3x3).\n","        t_pred (ndarray or Tensor): Predicted translation vector (3,).\n","        R_gt (ndarray or Tensor): Ground truth rotation matrix.\n","        t_gt (ndarray or Tensor): Ground truth translation vector.\n","        model_points (ndarray or Tensor): Nx3 3D model points.\n","\n","    Returns:\n","        float: Mean Euclidean distance between predicted and ground truth 3D points.\n","    \"\"\"\n","\n","    def to_np(x):\n","        return x.detach().cpu().numpy() if isinstance(x, torch.Tensor) else x\n","\n","    R_pred, t_pred = to_np(R_pred), to_np(t_pred)\n","    R_gt, t_gt = to_np(R_gt), to_np(t_gt)\n","    model_points = to_np(model_points)\n","\n","    pred_pts = model_points @ R_pred.T + t_pred\n","    gt_pts = model_points @ R_gt.T + t_gt\n","\n","    distances = np.linalg.norm(pred_pts - gt_pts, axis=1)\n","    return distances.mean()\n","\n","# ----------------------------------------------------\n","# Converts a batch of rotation matrices to quaternions\n","# ----------------------------------------------------\n","def matrix_to_quaternion_batch(rotation_matrix):\n","    \"\"\"\n","    Converts a batch of 3x3 rotation matrices to unit quaternions.\n","\n","    Args:\n","        rotation_matrix (Tensor): (B, 3, 3) batch of rotation matrices.\n","\n","    Returns:\n","        Tensor: (B, 4) batch of quaternions [w, x, y, z].\n","    \"\"\"\n","    if rotation_matrix.dim() == 2:\n","        rotation_matrix = rotation_matrix.unsqueeze(0)\n","\n","    batch_size = rotation_matrix.size(0)\n","    quaternions = torch.zeros(batch_size, 4, device=rotation_matrix.device)\n","\n","    trace = torch.diagonal(rotation_matrix, dim1=1, dim2=2).sum(dim=1)\n","    trace = torch.clamp(trace, min=-0.999)\n","\n","    s = torch.sqrt(1.0 + trace) / 2.0\n","    quaternions[:, 0] = s  # w\n","\n","    denom = 4.0 * s\n","    denom = torch.clamp(denom, min=1e-6)\n","\n","    quaternions[:, 1] = (rotation_matrix[:, 2, 1] - rotation_matrix[:, 1, 2]) / denom\n","    quaternions[:, 2] = (rotation_matrix[:, 0, 2] - rotation_matrix[:, 2, 0]) / denom\n","    quaternions[:, 3] = (rotation_matrix[:, 1, 0] - rotation_matrix[:, 0, 1]) / denom\n","\n","    return F.normalize(quaternions, dim=1)\n","\n","# ---------------------------------------------------\n","# Converts a batch of quaternions to rotation matrices\n","# ---------------------------------------------------\n","def quaternion_to_matrix_batch(quat):\n","    \"\"\"\n","    Converts a batch of quaternions [w, x, y, z] to rotation matrices.\n","\n","    Args:\n","        quat (Tensor): (B, 4) or (4,) tensor of quaternions.\n","\n","    Returns:\n","        Tensor: (B, 3, 3) or (3, 3) rotation matrices.\n","    \"\"\"\n","    if quat.dim() == 1:\n","        quat = quat.unsqueeze(0)\n","\n","    w, x, y, z = quat[:, 0], quat[:, 1], quat[:, 2], quat[:, 3]\n","    B = quat.size(0)\n","    R = torch.zeros((B, 3, 3), device=quat.device)\n","\n","    # Standard quaternion to matrix conversion formula.\n","    R[:, 0, 0] = 1 - 2 * (y**2 + z**2)\n","    R[:, 0, 1] = 2 * (x * y - z * w)\n","    R[:, 0, 2] = 2 * (x * z + y * w)\n","    R[:, 1, 0] = 2 * (x * y + z * w)\n","    R[:, 1, 1] = 1 - 2 * (x**2 + z**2)\n","    R[:, 1, 2] = 2 * (y * z - x * w)\n","    R[:, 2, 0] = 2 * (x * z - y * w)\n","    R[:, 2, 1] = 2 * (y * z + x * w)\n","    R[:, 2, 2] = 1 - 2 * (x**2 + y**2)\n","\n","    if quat.dim() == 1:\n","        R = R.squeeze(0)\n","\n","    return R\n","\n","# -----------------------------\n","# Quaternion-based loss function\n","# -----------------------------\n","def quaternion_loss(quat_pred, quat_gt):\n","    \"\"\"\n","    Computes quaternion loss that penalizes angular difference between orientations.\n","\n","    Args:\n","        quat_pred (Tensor): Predicted quaternions (B, 4).\n","        quat_gt (Tensor): Ground truth quaternions (B, 4).\n","\n","    Returns:\n","        Tensor: Scalar loss value.\n","    \"\"\"\n","    quat_pred = F.normalize(quat_pred, dim=1)\n","    quat_gt = F.normalize(quat_gt, dim=1)\n","\n","    dot = torch.sum(quat_pred * quat_gt, dim=1)\n","    dot = torch.clamp(dot, -1.0 + 1e-4, 1.0 - 1e-4)\n","\n","    return (1 - dot**2).mean()\n","\n","# -----------------------------------\n","# Computes angular error in degrees\n","# -----------------------------------\n","def quaternion_angular_error(q1, q2):\n","    \"\"\"\n","    Computes angular error between two quaternions in degrees.\n","\n","    Args:\n","        q1 (Tensor): Predicted quaternions (B, 4).\n","        q2 (Tensor): Ground truth quaternions (B, 4).\n","\n","    Returns:\n","        Tensor: Angular error per sample (B,)\n","    \"\"\"\n","    dot = torch.sum(q1 * q2, dim=1).clamp(-1.0, 1.0)\n","    dot = torch.abs(dot)  # Handle ¬±q ambiguity.\n","    angle = 2 * torch.acos(dot) * (180.0 / torch.pi)\n","    return angle\n","\n","# ------------------------------\n","# MSE loss for pose estimation\n","# ------------------------------\n","def computeMSE(rot_pred, t_pred, rot_gt, t_gt, quat=False,\n","               weight_xyz=(1.0, 1.0, 0.1), beta=1.0, print_mse=False):\n","    \"\"\"\n","    Computes Mean Squared Error loss for pose estimation.\n","\n","    Args:\n","        rot_pred (Tensor): Predicted rotation (quaternion or matrix).\n","        t_pred (Tensor): Predicted translation (B, 3).\n","        rot_gt (Tensor): Ground truth rotation (quaternion or matrix).\n","        t_gt (Tensor): Ground truth translation (B, 3).\n","        quat (bool): Whether to use quaternion loss.\n","        weight_xyz (tuple): Weights for (x, y, z) axes in translation loss.\n","        beta (float): Weighting factor for rotation loss.\n","        print_mse (bool): Whether to print detailed loss info.\n","\n","    Returns:\n","        Tensor: Combined pose loss.\n","    \"\"\"\n","    t_pred = torch.clamp(t_pred, min=1e-3)\n","    t_gt = torch.clamp(t_gt, min=1e-3)\n","\n","    x_loss = F.mse_loss(t_pred[:, 0], t_gt[:, 0]) * weight_xyz[0]\n","    y_loss = F.mse_loss(t_pred[:, 1], t_gt[:, 1]) * weight_xyz[1]\n","    z_loss = F.mse_loss(torch.log(t_pred[:, 2]), torch.log(t_gt[:, 2])) * weight_xyz[2]\n","    translation_loss = x_loss + y_loss + z_loss\n","\n","    if quat:\n","        rotation_loss = quaternion_loss(rot_pred, rot_gt)\n","    else:\n","        rot_diff = torch.bmm(rot_pred.transpose(1, 2), rot_gt)\n","        identity = torch.eye(3, device=rot_pred.device).unsqueeze(0).expand(rot_pred.size(0), -1, -1)\n","        rotation_loss = F.mse_loss(rot_diff, identity)\n","\n","    total_loss = translation_loss + beta * rotation_loss\n","\n","    if print_mse:\n","        print(f\"\\nX loss:           {x_loss:.6f}\")\n","        print(f\"Y loss:           {y_loss:.6f}\")\n","        print(f\"Z loss:           {z_loss:.6f}\")\n","        print(f\"Rotation loss:    {rotation_loss:.6f}\")\n","        print(f\"Total loss:       {total_loss:.6f}\")\n","        if quat:\n","            angle_deg = quaternion_angular_error(rot_pred, rot_gt).mean().item()\n","            print(f\"Angular error (deg): {angle_deg:.2f}\")\n","\n","    return total_loss\n","\n","# ------------------------------------\n","# Custom collate function for dataloader\n","# ------------------------------------\n","def flatten_collate_fn(batch):\n","    \"\"\"\n","    Collate function that flattens object entries from a batch of multi-object samples.\n","    Useful for object-wise training in 6D pose estimation.\n","\n","    Args:\n","        batch (list): List of samples, each containing a list of objects.\n","\n","    Returns:\n","        dict: Batched tensors of cropped RGBs, poses, IDs, intrinsics, and bboxes.\n","    \"\"\"\n","    flat_data = []\n","    for sample in batch:\n","        for obj in sample['objects']:\n","            flat_data.append({\n","                'rgb': obj['cropped_rgb'],\n","                'rotation': torch.tensor(obj['rotation'], dtype=torch.float32),\n","                'translation': torch.tensor(obj['translation'], dtype=torch.float32),\n","                'object_id': obj['object_id'],\n","                'bbox': obj['norm_bbox'].clone().detach(),\n","                'cropped_K': obj['cropped_K'].clone().detach()\n","            })\n","\n","    rgb = torch.stack([item['rgb'] for item in flat_data])\n","    rotation = torch.stack([item['rotation'] for item in flat_data])\n","    translation = torch.stack([item['translation'] for item in flat_data])\n","    object_ids = torch.tensor([item['object_id'] for item in flat_data], dtype=torch.int64)\n","    bbox = torch.stack([item['bbox'] for item in flat_data])\n","    cropped_K = torch.stack([item['cropped_K'] for item in flat_data])\n","\n","    return {\n","        'rgb': rgb,\n","        'rotation': rotation,\n","        'translation': translation,\n","        'object_id': object_ids,\n","        'norm_bbox': bbox,\n","        'cropped_K': cropped_K\n","    }"]},{"cell_type":"markdown","metadata":{"id":"ZbHqpO5TaAgq"},"source":["## Dataloaders"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":71402,"status":"ok","timestamp":1747485539584,"user":{"displayName":"Filip Nykvist","userId":"11309094825996781814"},"user_tz":-120},"id":"2ILaA8ppaC5E","outputId":"e0cdd788-7236-4bce-8cc2-fedf326e3aca"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading the entire LineMOD dataset.\n","Training dataset size: 12640\n","Testing dataset size: 3160\n","Training on 13 object types: ['01', '02', '04', '05', '06', '08', '09', '10', '11', '12', '13', '14', '15']\n"]}],"source":["from torch.utils.data import DataLoader\n","import os\n","import numpy as np\n","\n","# Path to the full dataset (not a single folder anymore).\n","dataset_root = os.path.join(full_project_path, 'dataset/LineMOD/Linemod_preprocessed/data')\n","models_root = os.path.join(full_project_path, 'dataset/LineMOD/Linemod_preprocessed/models')\n","\n","# What parts of the dataset would you like to include?\n","#folders = [1,2,3,4,5]\n","all_data = True\n","if all_data:\n","    print(\"Loading the entire LineMOD dataset.\")\n","else:\n","    print(f\"Loading data folder(s): {folders}\")\n","\n","# Defining the dataset splits.\n","train_dataset = PoseEstimationDataset(dataset_root,\n","                                      models_root,\n","                                      #folders=folders, # Comment out to train on the entire dataset.\n","                                      split='train')\n","\n","test_dataset = PoseEstimationDataset(dataset_root,\n","                                     models_root,\n","                                     #folders =folders, # Comment out to train on the entire dataset.\n","                                     split='test')\n","\n","print(f\"Training dataset size: {len(train_dataset)}\")\n","print(f\"Testing dataset size: {len(test_dataset)}\")\n","\n","# Optional: List objects present in training set.\n","_, orig_ids = train_dataset.getMappedIDs()\n","print(f\"Training on {len(orig_ids)} object types: {[f'{oid:02d}' for oid in sorted(orig_ids)]}\")\n","\n","# Define dataloaders.\n","num_workers = 2\n","batch_size = 32\n","\n","train_loader = DataLoader(\n","    train_dataset,\n","    batch_size=batch_size,\n","    shuffle=True,\n","    num_workers=num_workers,\n","    pin_memory=True,\n","    collate_fn=flatten_collate_fn\n",")\n","\n","test_loader = DataLoader(\n","    test_dataset,\n","    batch_size=batch_size,\n","    shuffle=False,\n","    num_workers=num_workers,\n","    pin_memory=True,\n","    collate_fn=flatten_collate_fn\n",")\n","\n","# Storing the object mapping.\n","train_dataset.save_mapping()"]},{"cell_type":"markdown","metadata":{"id":"B1_YULW7bevX"},"source":["## Functions for training and evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pyIh6C7gbjDJ"},"outputs":[],"source":["from tqdm.notebook import tqdm\n","\n","def train_model(epoch, model, train_loader, criterion, optimizer, device):\n","    \"\"\"\n","    Trains the PoseNet6D model for one epoch.\n","\n","    Args:\n","        epoch (int): Current epoch number.\n","        model (nn.Module): The model to train.\n","        train_loader (DataLoader): DataLoader for training data.\n","        criterion (callable): Loss function (not used here; MSE computed inline).\n","        optimizer (torch.optim.Optimizer): Optimizer for backpropagation.\n","        device (torch.device): Device to run training on (CPU/GPU).\n","\n","    Returns:\n","        nn.Module: Updated model after training.\n","    \"\"\"\n","    model.train()\n","    running_loss = 0.0\n","    total = 0\n","\n","    for batch_idx, data in enumerate(tqdm(train_loader, desc=f'Epoch {epoch}', leave=False, unit=\"batch\")):\n","\n","        # Load batch to device.\n","        crop_rgb = data['rgb'].to(device)\n","        t_gt = data['translation'].to(device)\n","        R_gt = data['rotation'].to(device)\n","        norm_bbox = data['norm_bbox'].to(device)\n","        cropped_K = data['cropped_K'].to(device)\n","        object_ids = data['object_id'].to(device)\n","\n","        # Forward pass: get predicted translation and quaternion.\n","        t_pred, quat_pred = model(crop_rgb, norm_bbox, cropped_K, object_ids)\n","\n","        # Convert GT rotation matrix to quaternion.\n","        quat_gt = matrix_to_quaternion_batch(R_gt)\n","\n","        # Dynamic loss weighting schedule per epoch.\n","        if epoch < 5:\n","            beta = 1\n","            weight_xyz = (0, 0, 0.1)\n","        elif epoch < 10:\n","            beta = 5\n","            weight_xyz = (0.5, 0.5, 0.4)\n","        else:\n","            beta = 10\n","            weight_xyz = (0.1, 0.1, 1)\n","\n","        # Compute combined loss (translation + rotation).\n","        loss = computeMSE(quat_pred, t_pred, quat_gt, t_gt,\n","                          quat=True, weight_xyz=weight_xyz, beta=beta)\n","\n","        # Skipping bad batches.\n","        if torch.isnan(loss) or torch.isinf(loss):\n","            print(f\"‚ö†Ô∏è Skipping batch {batch_idx} due to invalid loss (NaN or Inf)\")\n","            continue\n","\n","        # Backpropagation.\n","        optimizer.zero_grad()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","        total += t_gt.size(0)\n","\n","    train_loss = running_loss / len(train_loader)\n","    print(f'Epoch {epoch} | Loss: {train_loss:.6f} | LR: {optimizer.param_groups[0][\"lr\"]:.6f}')\n","    return model\n","\n","def evaluate_model(model, val_loader, dataset, device, track_per_object=False):\n","    \"\"\"\n","    Evaluates the model using ADD and MSE loss on the validation set.\n","\n","    Args:\n","        model (nn.Module): Trained model.\n","        val_loader (DataLoader): DataLoader for validation data.\n","        dataset (PoseEstimationDataset): Dataset object to access 3D models.\n","        device (torch.device): Device to use (CPU/GPU).\n","        track_per_object (bool): Whether to compute per-object ADD scores.\n","\n","    Returns:\n","        tuple: (avg_loss, avg_add) for the entire validation set.\n","    \"\"\"\n","    model.eval()\n","    running_loss = 0.0\n","    add_total = 0.0\n","    num_samples = 0\n","    first_round = True\n","\n","    # Loading the object ID mapping.\n","    idx_to_id = {v: k for k, v in PoseEstimationDataset.load_mapping(\"object_id_mapping.json\").items()}\n","\n","    # Per-object ADD stats (if enabled).\n","    add_per_object = {orig_id: [] for orig_id in idx_to_id.values()} if track_per_object else None\n","\n","    # Cache for loaded 3D model points per object.\n","    model_points_cache = {}\n","\n","    with torch.no_grad():\n","        for data in tqdm(val_loader, desc=\"Evaluating\", leave=False):\n","            crop_rgb = data['rgb'].to(device)\n","            t_gt = data['translation'].to(device)\n","            R_gt = data['rotation'].to(device)\n","            object_ids = data['object_id']\n","            norm_bbox = data['norm_bbox'].to(device)\n","            cropped_K = data['cropped_K'].to(device)\n","\n","            # Forward pass.\n","            t_pred, quat_pred = model(crop_rgb, norm_bbox, cropped_K, object_ids)\n","            quat_gt = matrix_to_quaternion_batch(R_gt)\n","            R_pred = quaternion_to_matrix_batch(quat_pred)\n","\n","            # Compute total loss.\n","            loss = computeMSE(quat_pred, t_pred, quat_gt, t_gt, quat=True, print_mse=first_round)\n","            first_round = False\n","            running_loss += loss.item()\n","\n","            # Compute ADD for each object in the batch.\n","            for i in range(crop_rgb.size(0)):\n","                mapped_id = int(object_ids[i])\n","                original_id = idx_to_id[mapped_id]\n","\n","                # Load and cache 3D model points.\n","                if original_id not in model_points_cache:\n","                    model_np = dataset.load_3D_model(original_id)\n","                    model_points_cache[original_id] = torch.tensor(model_np, dtype=torch.float32).to(device)\n","                model_points = model_points_cache[original_id]\n","\n","                # Computing and storing the ADD metric.\n","                add = computeADD(R_pred[i], t_pred[i], R_gt[i], t_gt[i], model_points)\n","                add_total += add\n","                num_samples += 1\n","\n","                # If tracking per object.\n","                if track_per_object:\n","                    add_per_object[original_id].append(add)\n","\n","    avg_loss = running_loss / len(val_loader)\n","    avg_add = add_total / num_samples\n","    print(f'Validation Loss: {avg_loss:.6f}, Avg ADD: {avg_add:.4f}')\n","\n","    # Print per-object ADD scores.\n","    if track_per_object:\n","        print(\"\\nPer-object ADD (mean):\")\n","        for obj_id, adds in sorted(add_per_object.items()):\n","            if adds:\n","                mean_add = np.mean(adds)\n","                print(f\"  Object {obj_id:02d}: ADD = {mean_add:.4f}\")\n","            else:\n","                print(f\"  Object {obj_id:02d}: No samples\")\n","\n","    return avg_loss, avg_add"]},{"cell_type":"markdown","metadata":{"id":"gyej-4XJ4ksF"},"source":["## Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QP6_J28o4mEH"},"outputs":[],"source":["import os\n","import torch\n","import matplotlib.pyplot as plt\n","from torch import nn, optim\n","from tqdm import tqdm\n","\n","# Flags to control where checkpoints are saved.\n","SAVE_LOCAL = True\n","SAVE_DRIVE = True\n","\n","def train_and_evaluate(model, train_loader, test_loader, train_dataset, test_dataset,\n","                       full_project_path, num_epochs=10, patience=5, start_epoch=1):\n","    \"\"\"\n","    Full training + evaluation routine for PoseNet6D.\n","    Handles checkpoints, early stopping, LR scheduling, and visualizations.\n","\n","    Args:\n","        model (nn.Module): PoseNet6D model.\n","        train_loader (DataLoader): DataLoader for training data.\n","        test_loader (DataLoader): DataLoader for test/validation data.\n","        train_dataset, test_dataset: Dataset objects (used for statistics and model access).\n","        full_project_path (str): Root path for saving files on Google Drive.\n","        num_epochs (int): Total number of epochs.\n","        patience (int): Early stopping patience on ADD.\n","        start_epoch (int): Epoch to start from (for resuming).\n","    \"\"\"\n","\n","    # === Defining checkpoint and model paths ===\n","    checkpoint_path_local = \"/content/checkpoint_OP.pth\"\n","    checkpoint_path_drive = os.path.join(full_project_path, \"models/checkpoint_OP.pth\")\n","    best_model_path_drive = os.path.join(full_project_path, \"models/best_posenet_OP.pt\")\n","    best_model_path_local = \"/content/best_posenet_OP.pt\"\n","\n","    # === Device setup ===\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    print(\"<<<<<<Using GPU>>>>>>\" if torch.cuda.is_available() else \"<<<<<<Using CPU>>>>>>\")\n","    model.to(device)\n","\n","    # === Optimizer and Learning Rate Scheduler ===\n","    optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad], lr=0.001, weight_decay=0.005)\n","    lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n","\n","    # === Checkpoint restore ===\n","    train_losses, add_losses = [], []\n","    best_add = float('inf')\n","\n","    # === Loading checkpoint if it's available ===\n","    if os.path.exists(checkpoint_path_local):\n","        checkpoint = torch.load(checkpoint_path_local, map_location=device, weights_only=False)\n","        model.load_state_dict(checkpoint['model'])\n","        optimizer.load_state_dict(checkpoint['optimizer'])\n","        best_add = checkpoint['best_add']\n","        train_losses = checkpoint['train_losses']\n","        add_losses = checkpoint['add_losses']\n","        start_epoch = checkpoint['epoch'] + 1\n","        print(f\"Resumed training from epoch {start_epoch}.\")\n","    else:\n","        print(\"No checkpoint found, starting from epoch 1.\")\n","\n","    # Early stopping counter.\n","    counter = 0\n","\n","    # === Training Loop ===\n","    for epoch in range(start_epoch, num_epochs + 1):\n","        print(f\"\\n--------- Starting Epoch {epoch}/{num_epochs} ---------\")\n","        print(f\">>>>>>>Current best ADD is {best_add:.4f}<<<<<<<<<\")\n","        print(f\"Invalid samples found in dataset: {train_dataset.nrInvalidObjects()}\")\n","\n","        # Train for one epoch.\n","        model = train_model(epoch, model, train_loader, computeMSE, optimizer, device)\n","\n","        # Validate on the test set.\n","        avg_loss, avg_add = evaluate_model(model, test_loader, test_dataset, device)\n","        train_losses.append(avg_loss)\n","        add_losses.append(avg_add)\n","\n","        # === Save the best model (based on ADD) ===\n","        if avg_add < best_add:\n","            best_add = avg_add\n","            counter = 0\n","            model.eval()\n","\n","            if SAVE_LOCAL:\n","                torch.save(model.state_dict(), best_model_path_local)\n","                print(f\"‚úÖ New best ADD: {avg_add:.4f} (saved model locally)\")\n","            if SAVE_DRIVE:\n","                torch.save(model.state_dict(), best_model_path_drive)\n","                print(f\"‚òÅÔ∏è New best ADD: {avg_add:.4f} (saved model on Google Drive)\")\n","        else:\n","            counter += 1\n","            if counter >= patience:\n","                print(\"‚èπ Early stopping triggered.\")\n","                break\n","\n","        # === Saving the checkpoint after each epoch ===\n","        checkpoint = {\n","            'epoch': epoch,\n","            'model': model.state_dict(),\n","            'optimizer': optimizer.state_dict(),\n","            'best_add': best_add,\n","            'train_losses': train_losses,\n","            'add_losses': add_losses,\n","            'version': 1\n","        }\n","        if SAVE_LOCAL:\n","            torch.save(checkpoint, checkpoint_path_local)\n","            print(\"üíæ Checkpoint saved locally.\")\n","        if SAVE_DRIVE:\n","            torch.save(checkpoint, checkpoint_path_drive)\n","            print(\"‚òÅÔ∏è Checkpoint saved on Google Drive.\")\n","\n","        # === Step LR scheduler based on the validation loss ===\n","        lr_scheduler.step(avg_loss)\n","        print(f\"Epoch {epoch}/{num_epochs} | Average ADD: {avg_add:.4f}\")\n","\n","    # === Plotting training and validation curves after finished training ===\n","    fig, ax1 = plt.subplots()\n","    ax2 = ax1.twinx()\n","\n","    ax1.plot(train_losses, 'g-', label='MSE Loss')\n","    ax2.plot(add_losses, 'b-', label='ADD')\n","\n","    ax1.set_xlabel('Epoch')\n","    ax1.set_ylabel('MSE Loss', color='g')\n","    ax2.set_ylabel('ADD (m)', color='b')\n","    ax1.set_title(\"Training Loss and Validation ADD\")\n","    ax1.grid(True)\n","\n","    # Combined legend for both axes.\n","    lines1, labels1 = ax1.get_legend_handles_labels()\n","    lines2, labels2 = ax2.get_legend_handles_labels()\n","    ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","    return model, train_losses, add_losses\n","\n","# Determining the number of objects in dataset.\n","_, num_objects = train_dataset.getMappedIDs()\n","num_obj = len(num_objects)\n","print(f\"This model will be trained to find {num_obj} object(s).\")\n","\n","# Initialize model and launch training.\n","model = PoseNet6D(num_objects=num_obj)\n","\n","train_and_evaluate(\n","    model=model,\n","    train_loader=train_loader,\n","    test_loader=test_loader,\n","    train_dataset=train_dataset,\n","    test_dataset=test_dataset,\n","    full_project_path=full_project_path,\n","    num_epochs=50,\n","    patience=10\n",")"]},{"cell_type":"markdown","metadata":{"id":"6_toHUHZQ1hg"},"source":["## Functions for plotting the results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W-_xRYM5Q237"},"outputs":[],"source":["import os\n","import cv2\n","import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torchvision.utils import save_image\n","\n","# === Drawing Functions ===\n","\n","def draw_model_projection(image, points, color, radius=1):\n","    \"\"\"\n","    Draws 2D projected 3D model points on the image.\n","\n","    Args:\n","        image (ndarray): Image to draw on.\n","        points (ndarray): 2D points (N, 2).\n","        color (tuple): BGR color.\n","        radius (int): Radius of circles to draw.\n","    \"\"\"\n","    for pt in points.astype(int):\n","        x, y = pt\n","        if 0 <= x < image.shape[1] and 0 <= y < image.shape[0]:\n","            cv2.circle(image, (x, y), radius, color, -1)\n","\n","def draw_legend(image, labels_colors):\n","    \"\"\"\n","    Draws a legend box on the image showing labels and colors.\n","\n","    Args:\n","        image (ndarray): Image to draw on.\n","        labels_colors (list): List of tuples: (label, BGR color).\n","    \"\"\"\n","    x, y, spacing = 10, 25, 25\n","    font_scale = 0.4\n","    text_thickness = 1\n","    box_width, box_height = 10, 10\n","    for i, (label, color) in enumerate(labels_colors):\n","        cv2.putText(image, label, (x + box_width + 5, y + i * spacing),\n","                    cv2.FONT_HERSHEY_SIMPLEX, font_scale, color, text_thickness)\n","        cv2.rectangle(image,\n","                      (x, y - box_height // 2 + i * spacing),\n","                      (x + box_width, y + box_height // 2 + i * spacing),\n","                      color, -1)\n","\n","def draw_axes(image, R, t, K, is_gt=False, axis_length=0.05, thickness=2):\n","    \"\"\"\n","    Draws 3D coordinate axes projected into the image.\n","\n","    Args:\n","        image (ndarray): Image to draw on.\n","        R (ndarray): 3x3 rotation matrix.\n","        t (ndarray): 3D translation vector.\n","        K (ndarray): Camera intrinsic matrix.\n","        is_gt (bool): Whether this is ground truth or prediction.\n","        axis_length (float): Length of each axis in meters.\n","        thickness (int): Line thickness.\n","\n","    Returns:\n","        ndarray: Modified image.\n","    \"\"\"\n","    axes_3d = np.array([\n","        [0, 0, 0],\n","        [axis_length, 0, 0],\n","        [0, axis_length, 0],\n","        [0, 0, axis_length]\n","    ], dtype=np.float32)\n","\n","    pts_2d = (K @ (axes_3d @ R.T + t).T).T\n","    pts_2d = pts_2d[:, :2] / pts_2d[:, 2:3]\n","    pts_2d = pts_2d.astype(int)\n","\n","    o = tuple(pts_2d[0])\n","    cv2.circle(image, o, 6, (255, 255, 255), -1)\n","\n","    # Use light colors for GT, strong for prediction\n","    color_map = [(0, 0, 255), (0, 255, 0), (255, 0, 0)] if not is_gt else \\\n","                [(100, 100, 255), (100, 255, 100), (255, 100, 100)]\n","    for i in range(1, 4):\n","        cv2.line(image, o, tuple(pts_2d[i]), color_map[i - 1], thickness)\n","\n","    return image\n","\n","def project(pts, R, t, K):\n","    \"\"\"\n","    Projects 3D points onto the 2D image plane using the given pose and intrinsics.\n","    \"\"\"\n","    proj = (K @ (pts @ R.T + t).T).T\n","    return proj[:, :2] / proj[:, 2:3]\n","\n","# === Pose Visualization for a Single Object ===\n","\n","def visualize_pose_prediction(obj, model, dataset, obj_id, device,\n","                              draw_axes_flag=False, draw_models=True, use_full_frame=True,\n","                              save_dir=None, save_prefix=\"result\", legend=False):\n","    \"\"\"\n","    Visualizes the predicted and ground truth poses for one object.\n","\n","    Args:\n","        obj (dict): One object entry from a dataset item.\n","        model (nn.Module): Trained pose estimation model.\n","        dataset: PoseEstimationDataset instance.\n","        obj_id (int): Mapped object ID.\n","        device: Device for inference.\n","        draw_axes_flag (bool): Draw X Y Z axes for pose.\n","        draw_models (bool): Project model points.\n","        use_full_frame (bool): Show full image view too.\n","        save_dir (str): Directory to save image (optional).\n","        save_prefix (str): Prefix for output filenames.\n","        legend (bool): Whether to draw the legend.\n","    \"\"\"\n","    # === Preparing the model inputs ===\n","    rgb = obj['cropped_rgb'].unsqueeze(0).to(device)\n","    norm_bbox = obj['norm_bbox'].unsqueeze(0).to(device)\n","    cropped_K = obj['cropped_K'].unsqueeze(0).to(device)\n","    obj_tensor = torch.tensor([obj_id], dtype=torch.long).to(device)\n","\n","    R_gt = obj['rotation']\n","    t_gt = obj['translation']\n","\n","    # === Inference ===\n","    with torch.no_grad():\n","        t_pred, quat_pred = model(rgb, norm_bbox, cropped_K, obj_tensor)\n","\n","    R_pred = quaternion_to_matrix_batch(quat_pred.detach()).squeeze().cpu().numpy()\n","    t_pred = t_pred.detach().squeeze().cpu().numpy()\n","\n","    # Compute angular error (GT quaternion).\n","    quat_gt = matrix_to_quaternion_batch(torch.tensor(R_gt, dtype=torch.float32).to(device))\n","    with torch.no_grad():\n","        ang_err = quaternion_angular_error(quat_pred, quat_gt).item()\n","\n","    # Get original object ID and model.\n","    original_id = dataset.idx_to_id[obj_id]\n","    model_points = dataset.load_3D_model(original_id)\n","\n","    # === Cropped image view ===\n","    crop_rgb = obj['cropped_rgb'].cpu().numpy().transpose(1, 2, 0)\n","    crop_rgb = ((crop_rgb * 0.229 + 0.485).clip(0, 1) * 255).astype(np.uint8)\n","    crop_rgb = cv2.cvtColor(crop_rgb, cv2.COLOR_RGB2BGR)\n","    cropped_K_np = cropped_K.squeeze().cpu().numpy()\n","\n","    vis_crop = crop_rgb.copy()\n","\n","    # Draw axes.\n","    if draw_axes_flag:\n","        vis_crop = draw_axes(vis_crop, R_pred, t_pred, cropped_K_np)\n","        vis_crop = draw_axes(vis_crop, R_gt, t_gt, cropped_K_np, is_gt=True)\n","\n","    # Draw 3d-models.\n","    if draw_models:\n","        draw_model_projection(vis_crop, project(model_points, R_gt, t_gt, cropped_K_np), (0, 255, 0))\n","        draw_model_projection(vis_crop, project(model_points, R_pred, t_pred, cropped_K_np), (0, 0, 255))\n","\n","    # Draw legend.\n","    if legend:\n","        draw_legend(vis_crop, [(\"GT\", (0, 255, 0)), (\"Pred\", (0, 0, 255))])\n","\n","    # Computing the ADD metric.\n","    add = computeADD(R_pred, t_pred, R_gt, t_gt, model_points)\n","    print(f\"\\n‚û°Ô∏è Mapped object ID {obj_id:02d}\")\n","    print(f\"ADD: {add:.4f} m | Angular Error: {ang_err:.2f}¬∞\")\n","\n","    # Showing cropped result.\n","    plt.figure()\n","    plt.imshow(cv2.cvtColor(vis_crop, cv2.COLOR_BGR2RGB))\n","    plt.title(f\"[Cropped] Obj {original_id:02d}\")\n","    plt.axis(\"off\")\n","    plt.show()\n","\n","    # Optionally save.\n","    if save_dir:\n","        os.makedirs(save_dir, exist_ok=True)\n","        cv2.imwrite(os.path.join(save_dir, f\"{save_prefix}_cropped_obj{obj_id:02d}.png\"), vis_crop)\n","\n","    # === Full image view ===\n","    if use_full_frame:\n","        full_image = cv2.cvtColor(np.array(obj['original_rgb']), cv2.COLOR_RGB2BGR)\n","        original_K = obj['original_K'].numpy()\n","\n","        if draw_axes_flag:\n","            full_image = draw_axes(full_image, R_pred, t_pred, original_K)\n","            full_image = draw_axes(full_image, R_gt, t_gt, original_K, is_gt=True)\n","        if draw_models:\n","            draw_model_projection(full_image, project(model_points, R_gt, t_gt, original_K), (0, 255, 0))\n","            draw_model_projection(full_image, project(model_points, R_pred, t_pred, original_K), (0, 0, 255))\n","        if legend:\n","            draw_legend(full_image, [(\"GT\", (0, 255, 0)), (\"Pred\", (0, 0, 255))])\n","\n","        plt.figure()\n","        plt.imshow(cv2.cvtColor(full_image, cv2.COLOR_BGR2RGB))\n","        plt.title(f\"[Full Frame] Obj {original_id:02d}\")\n","        plt.axis(\"off\")\n","        plt.show()\n","\n","        if save_dir:\n","            cv2.imwrite(os.path.join(save_dir, f\"{save_prefix}_fullframe_obj{obj_id:02d}.png\"), full_image)\n","\n","# === Visualization Launcher for a Dataset Item ===\n","\n","def run_visualization(model, dataset, device, target_obj_ids,\n","                      img_idx=0, save_dir=None, draw_axes=False,\n","                      draw_legend=False, draw_models=False):\n","    \"\"\"\n","    Runs visualization on one image sample and selected object IDs.\n","\n","    Args:\n","        model (nn.Module): Trained pose model.\n","        dataset: Dataset containing objects.\n","        device: Device for inference.\n","        target_obj_ids (list): Mapped object IDs to visualize.\n","        img_idx (int): Index of the image in the dataset.\n","        save_dir (str): Output directory for saving visualizations.\n","        draw_axes (bool): Whether to draw pose axes.\n","        draw_legend (bool): Whether to draw a legend.\n","        draw_models (bool): Whether to project model points.\n","    \"\"\"\n","    model.eval()\n","    idx_to_id = {v: k for k, v in dataset.id_to_idx.items()}\n","\n","    with torch.no_grad():\n","        data_item = dataset[img_idx]\n","        found_obj_ids = set()\n","\n","        # Inject shared data into each object entry.\n","        for obj in data_item['objects']:\n","            obj_id = obj['object_id']\n","            if obj_id in target_obj_ids:\n","                found_obj_ids.add(obj_id)\n","                obj['original_rgb'] = data_item['original_rgb']\n","                obj['original_K'] = data_item['original_K']\n","\n","                visualize_pose_prediction(\n","                    obj, model, dataset, obj_id, device,\n","                    draw_axes_flag=draw_axes,\n","                    draw_models=draw_models,\n","                    legend=draw_legend,\n","                    save_dir=save_dir,\n","                    save_prefix=f\"img{img_idx:03d}_obj{obj_id:02d}\"\n","                )\n","\n","        # Warn if some requested object IDs were not present in this sample.\n","        missing_ids = set(target_obj_ids) - found_obj_ids\n","        if missing_ids:\n","            missing_original_ids = sorted([idx_to_id[mapped_id] for mapped_id in missing_ids])\n","            print(f\"‚ö†Ô∏è Note: These object IDs (mapped) were not present in image {img_idx}: {sorted(missing_ids)}\")"]},{"cell_type":"markdown","source":["## Plotting the results"],"metadata":{"id":"2nz2cPzV29bo"}},{"cell_type":"code","source":["# --- Dataset & Paths ---\n","dataset_root = os.path.join(full_project_path, 'dataset/LineMOD/Linemod_preprocessed/data')\n","models_root = os.path.join(full_project_path, \"dataset/LineMOD/Linemod_preprocessed/models\")\n","\n","# Load a subset (e.g., objects 1 and 13)\n","folders = [2] # List of folder [1,2,3].\n","dataset = PoseEstimationDataset(dataset_root,\n","                                models_root,\n","                                folders=folders)"],"metadata":{"id":"UrmjxppEp4H2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Printing all original object IDs found in the dataset.\n","print(\"‚úÖ Object IDs in dataset:\", dataset.object_ids)\n","\n","# Retrieving mapped internal IDs (used during training).\n","all_mapped_ids, _ = dataset.getMappedIDs()\n","print(\"‚úÖ Mapped IDs:\", all_mapped_ids)\n","\n","# === Select which object IDs you want to visualize ===\n","# Can be all (1‚Äì15) or a subset like [13]\n","original_ids_to_visualize = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n","# original_ids_to_visualize = [13]  # Uncomment to debug/visualize only one object.\n","\n","# === Model Setup ===\n","# Adjust num_objects to match what the model was trained on.\n","num_objects = len(original_ids_to_visualize)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Initialize and load the trained model.\n","model = PoseNet6D(num_objects=13)\n","model.load_state_dict(torch.load('/content/best_posenet_OP.pt',\n","                                 map_location=device))\n","model.to(device)\n","\n","# === Visualization ===\n","# Convert selected original object IDs to internal mapped indices.\n","mapped_ids, _ = dataset.getMappedIDs(original_ids_to_visualize)\n","\n","img_idx = 15  # Dataset index of the image you want to visualize.\n","save_dir = \"/content/visualizations\"  # Where to save the result images.\n","\n","# === Running the pose visualization ===\n","run_visualization(\n","    model=model,\n","    dataset=dataset,\n","    device=device,\n","    target_obj_ids=mapped_ids,  # Use mapped IDs for internal consistency.\n","    img_idx=img_idx,            # Visualize this specific sample.\n","    save_dir=save_dir,\n","    draw_axes=True,             # Show 3D pose axes.\n","    draw_legend=False,          # Skip legend overlay.\n","    draw_models=False           # Skip projecting 3D model points.\n",")"],"metadata":{"id":"V47LjZSf3AXX"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["Vu8EdO6H3r5T","q9iQ9GE_3wQG","KPyo8_eyCgyr","87dUPITNvUGI","keS9PBYASWjI","ZbHqpO5TaAgq","B1_YULW7bevX","gyej-4XJ4ksF","6_toHUHZQ1hg","2nz2cPzV29bo"],"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyN2rJ8LLr7GqdLkMSotTMwz"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
